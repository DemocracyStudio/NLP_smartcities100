{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan : Data Cleaning / Feature Extraction. \n",
    "\n",
    "Tweet-level features : \n",
    "    - delete null rows\n",
    "    - tweet length / num of words for each tweet / number of sentences / average length of words\n",
    "    - vader sentiment (neg/neu/pos/compound)\n",
    "    - number of likes\n",
    "\n",
    "City-level features :\n",
    "    - num of tweets\n",
    "    - avg tweet length / avg num words per tweet/ average number of sentences / average word length / average sentiment scores (neg/neu/pos/compound) \n",
    "    - num of words into concatenated tweets\n",
    "    - weight of thematic BoW\n",
    "    - extract features from counts of stopwords, ponctuation ...\n",
    "    - convert emoji, emoticons into words\n",
    "    - nlp tasks (expand contractions, remove digits, @mentions, url, punctuation, tokenizing, cleaning stopwords, useless, lemmatize, num clean tokens) \n",
    "    - part of speech tagging on tokens\n",
    "    - most frequent 100 tokens extraction\n",
    "    - sentiment analysis vader\n",
    "    - sentiment polarity and subjectivity with textblob\n",
    "    - countvectorizer with scikit learn\n",
    "    - tf-idf with scikit learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import *\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "\n",
    "# algorithms\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartcities100 = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/smartcities100/smartcities100 tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetDate</th>\n",
       "      <th>content</th>\n",
       "      <th>twitterProfile</th>\n",
       "      <th>tweetUrl</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>query</th>\n",
       "      <th>rank2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Jan 17 10:57:51 +0000 2021</td>\n",
       "      <td>A Delegation from  @aau_ae visited @BurjeelMed...</td>\n",
       "      <td>https://twitter.com/Atatreh</td>\n",
       "      <td>https://twitter.com/Atatreh/status/13507592313...</td>\n",
       "      <td>2021-01-21T13:59:48.147Z</td>\n",
       "      <td>#AbuDhabi</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thu Jan 14 17:40:03 +0000 2021</td>\n",
       "      <td>Wizz Air #AbuDhabi is set to launch flights to...</td>\n",
       "      <td>https://twitter.com/UAE_Forsan</td>\n",
       "      <td>https://twitter.com/UAE_Forsan/status/13497732...</td>\n",
       "      <td>2021-01-21T13:59:48.147Z</td>\n",
       "      <td>#AbuDhabi</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wed Jan 20 20:25:02 +0000 2021</td>\n",
       "      <td>What a great grappling exchange by both man #A...</td>\n",
       "      <td>https://twitter.com/RdosAnjosMMA</td>\n",
       "      <td>https://twitter.com/RdosAnjosMMA/status/135198...</td>\n",
       "      <td>2021-01-21T13:59:48.147Z</td>\n",
       "      <td>#AbuDhabi</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sun Jan 17 16:55:17 +0000 2021</td>\n",
       "      <td>Totally worth the 3.5 hour drive to #AbuDhabi ...</td>\n",
       "      <td>https://twitter.com/zoomnclick</td>\n",
       "      <td>https://twitter.com/zoomnclick/status/13508491...</td>\n",
       "      <td>2021-01-21T13:59:48.147Z</td>\n",
       "      <td>#AbuDhabi</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon Jan 18 04:44:44 +0000 2021</td>\n",
       "      <td>Good morning #AbuDhabi Ireland 🇮🇪 vs UAE 🇦🇪 fi...</td>\n",
       "      <td>https://twitter.com/ChTahirmehmood</td>\n",
       "      <td>https://twitter.com/ChTahirmehmood/status/1351...</td>\n",
       "      <td>2021-01-21T13:59:48.147Z</td>\n",
       "      <td>#AbuDhabi</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        tweetDate  \\\n",
       "0  Sun Jan 17 10:57:51 +0000 2021   \n",
       "1  Thu Jan 14 17:40:03 +0000 2021   \n",
       "2  Wed Jan 20 20:25:02 +0000 2021   \n",
       "3  Sun Jan 17 16:55:17 +0000 2021   \n",
       "4  Mon Jan 18 04:44:44 +0000 2021   \n",
       "\n",
       "                                             content  \\\n",
       "0  A Delegation from  @aau_ae visited @BurjeelMed...   \n",
       "1  Wizz Air #AbuDhabi is set to launch flights to...   \n",
       "2  What a great grappling exchange by both man #A...   \n",
       "3  Totally worth the 3.5 hour drive to #AbuDhabi ...   \n",
       "4  Good morning #AbuDhabi Ireland 🇮🇪 vs UAE 🇦🇪 fi...   \n",
       "\n",
       "                       twitterProfile  \\\n",
       "0         https://twitter.com/Atatreh   \n",
       "1      https://twitter.com/UAE_Forsan   \n",
       "2    https://twitter.com/RdosAnjosMMA   \n",
       "3      https://twitter.com/zoomnclick   \n",
       "4  https://twitter.com/ChTahirmehmood   \n",
       "\n",
       "                                            tweetUrl  \\\n",
       "0  https://twitter.com/Atatreh/status/13507592313...   \n",
       "1  https://twitter.com/UAE_Forsan/status/13497732...   \n",
       "2  https://twitter.com/RdosAnjosMMA/status/135198...   \n",
       "3  https://twitter.com/zoomnclick/status/13508491...   \n",
       "4  https://twitter.com/ChTahirmehmood/status/1351...   \n",
       "\n",
       "                  timestamp      query  rank2020  \n",
       "0  2021-01-21T13:59:48.147Z  #AbuDhabi        42  \n",
       "1  2021-01-21T13:59:48.147Z  #AbuDhabi        42  \n",
       "2  2021-01-21T13:59:48.147Z  #AbuDhabi        42  \n",
       "3  2021-01-21T13:59:48.147Z  #AbuDhabi        42  \n",
       "4  2021-01-21T13:59:48.147Z  #AbuDhabi        42  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smartcities100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many smartcities are there in my dataset ?\n",
    "smartcities100['query'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will append my 3 case studies Taipei, Tallinn, Tel Aviv which were stored in dedicated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "taipei = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/#Taipei.csv')\n",
    "telaviv = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/#Telaviv.csv')\n",
    "tallinn = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/#Tallinn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranking2020 column is missing on these 3 files. Will add it before merging the files in the same dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "taipei['rank2020'] = '8'\n",
    "telaviv['rank2020'] = '50'\n",
    "tallinn['rank2020'] = '59'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartcities100 = pd.concat([smartcities100, taipei,telaviv,tallinn], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110862, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smartcities100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smartcities100['query'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#telaviv         17612\n",
       "#Taipei          17337\n",
       "#Tallinn          6295\n",
       "#Brisbane         2080\n",
       "#Manchester       1877\n",
       "#Bilbao           1338\n",
       "#Zaragoza         1290\n",
       "#Montreal         1231\n",
       "#Vancouver        1187\n",
       "#SanFrancisco     1022\n",
       "#Bologna          1000\n",
       "#Rome             1000\n",
       "#Athens           1000\n",
       "#Bengaluru        1000\n",
       "#Oslo             1000\n",
       "#Berlin           1000\n",
       "#Toronto          1000\n",
       "#Mumbai           1000\n",
       "#Amsterdam        1000\n",
       "#Bogota           1000\n",
       "#Bangkok          1000\n",
       "#Munich           1000\n",
       "#HongKong          999\n",
       "#Helsinki          998\n",
       "#Lagos             991\n",
       "#Dublin            974\n",
       "#Newcastle         973\n",
       "#Barcelona         956\n",
       "#Hyderabad         946\n",
       "#Santiago          939\n",
       "#BuenosAires       932\n",
       "#Nairobi           918\n",
       "#SaoPaulo          913\n",
       "#Chicago           907\n",
       "#Madrid            903\n",
       "#Moscow            898\n",
       "#CapeTown          886\n",
       "#Boston            868\n",
       "#Melbourne         866\n",
       "#London            858\n",
       "#LosAngeles        856\n",
       "#Medellin          850\n",
       "#AbuDhabi          832\n",
       "#Abuja             828\n",
       "#Rotterdam         813\n",
       "#Vienna            799\n",
       "#Philadelphia      786\n",
       "#NewDelhi          761\n",
       "#Paris             760\n",
       "#Dubai             733\n",
       "#Tokyo             696\n",
       "#Cairo             671\n",
       "#Stockholm         660\n",
       "#MexicoCity        654\n",
       "#Milan             600\n",
       "#Lisbon            600\n",
       "#Nanjing           593\n",
       "#Phoenix           592\n",
       "#Beijing           592\n",
       "#RiodeJaneiro      590\n",
       "#Denver            585\n",
       "#Geneva            578\n",
       "#Singapore         567\n",
       "#Copenhaguen       565\n",
       "#Lyon              548\n",
       "#Auckland          548\n",
       "#Seattle           546\n",
       "#HoChiMinh         539\n",
       "#Bratislava        520\n",
       "#KualaLumpur       499\n",
       "#Zurich            485\n",
       "#Sydney            483\n",
       "#Dusseldorf        481\n",
       "#Tianjin           480\n",
       "#Prague            473\n",
       "#Budapest          465\n",
       "#Seoul             455\n",
       "#WashingtonDC      425\n",
       "#Krakow            423\n",
       "#StPetersburg      422\n",
       "#Zhuhai            422\n",
       "#Marseille         396\n",
       "#Riyadh            358\n",
       "#TheHague          349\n",
       "#Shenzhen          334\n",
       "#Hamburg           324\n",
       "#Shanghai          314\n",
       "#Ankara            313\n",
       "#Chengdu           286\n",
       "#Warsaw            284\n",
       "#Manila            268\n",
       "#Brussels          261\n",
       "#Osaka             237\n",
       "#Hanoi             236\n",
       "#Jakarta           210\n",
       "#Kiev              202\n",
       "#New York          191\n",
       "#Sofia             187\n",
       "#Chongqing         168\n",
       "#Guangzhou         167\n",
       "#Hangzhou          145\n",
       "#Rabat             138\n",
       "#Birmingham        131\n",
       "#Bucharest         126\n",
       "#Hanover           117\n",
       "#Makassar          112\n",
       "#Gothenburg         84\n",
       "#Medan              71\n",
       "#Busan              64\n",
       "Name: query, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smartcities100['query'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetDate</th>\n",
       "      <th>content</th>\n",
       "      <th>twitterProfile</th>\n",
       "      <th>tweetUrl</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>query</th>\n",
       "      <th>rank2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Jan 17 10:57:51 +0000 2021</td>\n",
       "      <td>A Delegation from  @aau_ae visited @BurjeelMed...</td>\n",
       "      <td>https://twitter.com/Atatreh</td>\n",
       "      <td>https://twitter.com/Atatreh/status/13507592313...</td>\n",
       "      <td>2021-01-21T13:59:48.147Z</td>\n",
       "      <td>#AbuDhabi</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thu Jan 14 17:40:03 +0000 2021</td>\n",
       "      <td>Wizz Air #AbuDhabi is set to launch flights to...</td>\n",
       "      <td>https://twitter.com/UAE_Forsan</td>\n",
       "      <td>https://twitter.com/UAE_Forsan/status/13497732...</td>\n",
       "      <td>2021-01-21T13:59:48.147Z</td>\n",
       "      <td>#AbuDhabi</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wed Jan 20 20:25:02 +0000 2021</td>\n",
       "      <td>What a great grappling exchange by both man #A...</td>\n",
       "      <td>https://twitter.com/RdosAnjosMMA</td>\n",
       "      <td>https://twitter.com/RdosAnjosMMA/status/135198...</td>\n",
       "      <td>2021-01-21T13:59:48.147Z</td>\n",
       "      <td>#AbuDhabi</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sun Jan 17 16:55:17 +0000 2021</td>\n",
       "      <td>Totally worth the 3.5 hour drive to #AbuDhabi ...</td>\n",
       "      <td>https://twitter.com/zoomnclick</td>\n",
       "      <td>https://twitter.com/zoomnclick/status/13508491...</td>\n",
       "      <td>2021-01-21T13:59:48.147Z</td>\n",
       "      <td>#AbuDhabi</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon Jan 18 04:44:44 +0000 2021</td>\n",
       "      <td>Good morning #AbuDhabi Ireland 🇮🇪 vs UAE 🇦🇪 fi...</td>\n",
       "      <td>https://twitter.com/ChTahirmehmood</td>\n",
       "      <td>https://twitter.com/ChTahirmehmood/status/1351...</td>\n",
       "      <td>2021-01-21T13:59:48.147Z</td>\n",
       "      <td>#AbuDhabi</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        tweetDate  \\\n",
       "0  Sun Jan 17 10:57:51 +0000 2021   \n",
       "1  Thu Jan 14 17:40:03 +0000 2021   \n",
       "2  Wed Jan 20 20:25:02 +0000 2021   \n",
       "3  Sun Jan 17 16:55:17 +0000 2021   \n",
       "4  Mon Jan 18 04:44:44 +0000 2021   \n",
       "\n",
       "                                             content  \\\n",
       "0  A Delegation from  @aau_ae visited @BurjeelMed...   \n",
       "1  Wizz Air #AbuDhabi is set to launch flights to...   \n",
       "2  What a great grappling exchange by both man #A...   \n",
       "3  Totally worth the 3.5 hour drive to #AbuDhabi ...   \n",
       "4  Good morning #AbuDhabi Ireland 🇮🇪 vs UAE 🇦🇪 fi...   \n",
       "\n",
       "                       twitterProfile  \\\n",
       "0         https://twitter.com/Atatreh   \n",
       "1      https://twitter.com/UAE_Forsan   \n",
       "2    https://twitter.com/RdosAnjosMMA   \n",
       "3      https://twitter.com/zoomnclick   \n",
       "4  https://twitter.com/ChTahirmehmood   \n",
       "\n",
       "                                            tweetUrl  \\\n",
       "0  https://twitter.com/Atatreh/status/13507592313...   \n",
       "1  https://twitter.com/UAE_Forsan/status/13497732...   \n",
       "2  https://twitter.com/RdosAnjosMMA/status/135198...   \n",
       "3  https://twitter.com/zoomnclick/status/13508491...   \n",
       "4  https://twitter.com/ChTahirmehmood/status/1351...   \n",
       "\n",
       "                  timestamp      query rank2020  \n",
       "0  2021-01-21T13:59:48.147Z  #AbuDhabi       42  \n",
       "1  2021-01-21T13:59:48.147Z  #AbuDhabi       42  \n",
       "2  2021-01-21T13:59:48.147Z  #AbuDhabi       42  \n",
       "3  2021-01-21T13:59:48.147Z  #AbuDhabi       42  \n",
       "4  2021-01-21T13:59:48.147Z  #AbuDhabi       42  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smartcities100.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. extract some tweet-level features\n",
    "- tweet length for each tweet\n",
    "- number of words for each tweet\n",
    "- number of sentences for each tweet\n",
    "- average word length for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many null values in content column ?\n",
    "smartcities100['content'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only rows when content column value is not null\n",
    "smartcities100 = smartcities100[smartcities100['content'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smartcities100['content'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column with tweet length for each tweet\n",
    "smartcities100['tweet_len'] = smartcities100['content'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column with number of words for each tweet\n",
    "smartcities100['num_words'] = smartcities100['content'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column with number of stopwords for each tweet\n",
    "stop = stopwords.words('english')\n",
    "smartcities100['stopwords'] = smartcities100['content'].apply(lambda x: len([x for x in x.split() if x in stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a column with number of sentences for each tweet\n",
    "smartcities100['num_sentences'] = smartcities100['content'].apply(lambda x: len(re.split( '~ ...' ,'~'.join(x.split('.')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function for finding average length of words per tweet\n",
    "def get_avg_word_len(x):\n",
    "    words = x.split()\n",
    "    word_len = 0\n",
    "    for word in words:\n",
    "        word_len = word_len + len(word)\n",
    "        \n",
    "    return word_len / len(words)\n",
    "\n",
    "# add a column with average word length for each tweet\n",
    "smartcities100['avg_word_len'] = smartcities100['content'].apply(lambda x: get_avg_word_len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fuction to find the number of punctuation for each tweet\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return count\n",
    "\n",
    "# add a column with number of punctuation for each tweet\n",
    "smartcities100['punctuation'] = smartcities100['content'].apply(lambda x: count_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of hashtag per tweet\n",
    "smartcities100['hashtags'] = smartcities100['content'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of numerics per tweet\n",
    "smartcities100['numerics'] = smartcities100['content'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of uppercase words\n",
    "smartcities100['upper'] = smartcities100['content'].apply(lambda x: len([x for x in x.split() if x.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use vader sentiment to get % of pos, neg, neu in new columns\n",
    "analyzer= SentimentIntensityAnalyzer()\n",
    "\n",
    "smartcities100['neg'] = [analyzer.polarity_scores(v)['neg'] for v in smartcities100['content']]\n",
    "smartcities100['neu'] = [analyzer.polarity_scores(v)['neu'] for v in smartcities100['content']]\n",
    "smartcities100['pos'] = [analyzer.polarity_scores(v)['pos'] for v in smartcities100['content']]\n",
    "smartcities100['compound'] = [analyzer.polarity_scores(v)['compound'] for v in smartcities100['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment polarity and subjectivity with textblob\n",
    "smartcities100['polarity_tweet'] = smartcities100['content'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "smartcities100['subjectivity_tweet'] = smartcities100['content'].apply(lambda x: TextBlob(x).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find number of likes for each tweet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import file with likes received per tweet\n",
    "likes_export = pd.read_csv('/Users/juliencarbonnell/Desktop/sc100_likes_export.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# add number of likes per tweet\n",
    "smartcities100['likes'] = likes_export['likesCount'] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetDate</th>\n",
       "      <th>content</th>\n",
       "      <th>twitterProfile</th>\n",
       "      <th>tweetUrl</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>query</th>\n",
       "      <th>rank2020</th>\n",
       "      <th>tweet_len</th>\n",
       "      <th>num_words</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>punctuation</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>polarity_tweet</th>\n",
       "      <th>subjectivity_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Jan 17 10:57:51 +0000 2021</td>\n",
       "      <td>A Delegation from  @aau_ae visited @BurjeelMed...</td>\n",
       "      <td>https://twitter.com/Atatreh</td>\n",
       "      <td>https://twitter.com/Atatreh/status/13507592313...</td>\n",
       "      <td>2021-01-21T13:59:48.147Z</td>\n",
       "      <td>#AbuDhabi</td>\n",
       "      <td>42</td>\n",
       "      <td>303</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>6.973684</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.5267</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thu Jan 14 17:40:03 +0000 2021</td>\n",
       "      <td>Wizz Air #AbuDhabi is set to launch flights to...</td>\n",
       "      <td>https://twitter.com/UAE_Forsan</td>\n",
       "      <td>https://twitter.com/UAE_Forsan/status/13497732...</td>\n",
       "      <td>2021-01-21T13:59:48.147Z</td>\n",
       "      <td>#AbuDhabi</td>\n",
       "      <td>42</td>\n",
       "      <td>87</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wed Jan 20 20:25:02 +0000 2021</td>\n",
       "      <td>What a great grappling exchange by both man #A...</td>\n",
       "      <td>https://twitter.com/RdosAnjosMMA</td>\n",
       "      <td>https://twitter.com/RdosAnjosMMA/status/135198...</td>\n",
       "      <td>2021-01-21T13:59:48.147Z</td>\n",
       "      <td>#AbuDhabi</td>\n",
       "      <td>42</td>\n",
       "      <td>53</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sun Jan 17 16:55:17 +0000 2021</td>\n",
       "      <td>Totally worth the 3.5 hour drive to #AbuDhabi ...</td>\n",
       "      <td>https://twitter.com/zoomnclick</td>\n",
       "      <td>https://twitter.com/zoomnclick/status/13508491...</td>\n",
       "      <td>2021-01-21T13:59:48.147Z</td>\n",
       "      <td>#AbuDhabi</td>\n",
       "      <td>42</td>\n",
       "      <td>114</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.388889</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.6590</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon Jan 18 04:44:44 +0000 2021</td>\n",
       "      <td>Good morning #AbuDhabi Ireland 🇮🇪 vs UAE 🇦🇪 fi...</td>\n",
       "      <td>https://twitter.com/ChTahirmehmood</td>\n",
       "      <td>https://twitter.com/ChTahirmehmood/status/1351...</td>\n",
       "      <td>2021-01-21T13:59:48.147Z</td>\n",
       "      <td>#AbuDhabi</td>\n",
       "      <td>42</td>\n",
       "      <td>91</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        tweetDate  \\\n",
       "0  Sun Jan 17 10:57:51 +0000 2021   \n",
       "1  Thu Jan 14 17:40:03 +0000 2021   \n",
       "2  Wed Jan 20 20:25:02 +0000 2021   \n",
       "3  Sun Jan 17 16:55:17 +0000 2021   \n",
       "4  Mon Jan 18 04:44:44 +0000 2021   \n",
       "\n",
       "                                             content  \\\n",
       "0  A Delegation from  @aau_ae visited @BurjeelMed...   \n",
       "1  Wizz Air #AbuDhabi is set to launch flights to...   \n",
       "2  What a great grappling exchange by both man #A...   \n",
       "3  Totally worth the 3.5 hour drive to #AbuDhabi ...   \n",
       "4  Good morning #AbuDhabi Ireland 🇮🇪 vs UAE 🇦🇪 fi...   \n",
       "\n",
       "                       twitterProfile  \\\n",
       "0         https://twitter.com/Atatreh   \n",
       "1      https://twitter.com/UAE_Forsan   \n",
       "2    https://twitter.com/RdosAnjosMMA   \n",
       "3      https://twitter.com/zoomnclick   \n",
       "4  https://twitter.com/ChTahirmehmood   \n",
       "\n",
       "                                            tweetUrl  \\\n",
       "0  https://twitter.com/Atatreh/status/13507592313...   \n",
       "1  https://twitter.com/UAE_Forsan/status/13497732...   \n",
       "2  https://twitter.com/RdosAnjosMMA/status/135198...   \n",
       "3  https://twitter.com/zoomnclick/status/13508491...   \n",
       "4  https://twitter.com/ChTahirmehmood/status/1351...   \n",
       "\n",
       "                  timestamp      query rank2020  tweet_len  num_words  \\\n",
       "0  2021-01-21T13:59:48.147Z  #AbuDhabi       42        303         38   \n",
       "1  2021-01-21T13:59:48.147Z  #AbuDhabi       42         87         12   \n",
       "2  2021-01-21T13:59:48.147Z  #AbuDhabi       42         53          9   \n",
       "3  2021-01-21T13:59:48.147Z  #AbuDhabi       42        114         18   \n",
       "4  2021-01-21T13:59:48.147Z  #AbuDhabi       42         91         12   \n",
       "\n",
       "   stopwords  num_sentences  avg_word_len  punctuation  hashtags  numerics  \\\n",
       "0         10              2      6.973684           14         2         0   \n",
       "1          3              1      6.333333            8         2         0   \n",
       "2          3              1      5.000000            1         1         0   \n",
       "3          5              1      5.388889           10         2         1   \n",
       "4          0              1      6.666667            7         2         0   \n",
       "\n",
       "   upper    neg    neu    pos  compound  polarity_tweet  subjectivity_tweet  \n",
       "0      2  0.104  0.849  0.047   -0.5267            0.40                0.60  \n",
       "1      0  0.000  1.000  0.000    0.0000            0.00                0.00  \n",
       "2      0  0.000  0.661  0.339    0.6249            0.80                0.75  \n",
       "3      1  0.000  0.748  0.252    0.6590            0.30                0.10  \n",
       "4      1  0.000  0.791  0.209    0.4404            0.35                0.80  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the columns of my dataset\n",
    "smartcities100.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save new .csv file with lastly created columns\n",
    "smartcities100.to_csv('smartcities100_.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now my dataset is complete and we can see that my 3 case studies are bigger samples from the other cities. This unbalance is not annoying since I will work on means and standard deviations values. It gives my samples more proof when compared to the whole population. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extract city-level features \n",
    "- create a dataframe with city_id as index\n",
    "- use groupby on the 'query' column \n",
    "- aggregate city-level features from the tweet-level dataframe \n",
    "- take the weight of smartcityBoWs for each city\n",
    "\n",
    "## Create a new dataframe to store features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the unique values from 'query' column as index in the new dataframe\n",
    "index = smartcities100['query'].unique()\n",
    "columns = []\n",
    "\n",
    "# create a new DataFrame\n",
    "sc100_features = pd.DataFrame(index=index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#AbuDhabi</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Abuja</th>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Amsterdam</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Ankara</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Athens</th>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            min\n",
       "#AbuDhabi    42\n",
       "#Abuja      107\n",
       "#Amsterdam    9\n",
       "#Ankara      57\n",
       "#Athens      99"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agg rank2020 column\n",
    "sc100_features = pd.concat([sc100_features, smartcities100.groupby('query').rank2020.agg(['min'])], axis=1)\n",
    "sc100_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#AbuDhabi</th>\n",
       "      <td>42</td>\n",
       "      <td>832.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Abuja</th>\n",
       "      <td>107</td>\n",
       "      <td>828.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Amsterdam</th>\n",
       "      <td>9</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Ankara</th>\n",
       "      <td>57</td>\n",
       "      <td>313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Athens</th>\n",
       "      <td>99</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            min   query\n",
       "#AbuDhabi    42   832.0\n",
       "#Abuja      107   828.0\n",
       "#Amsterdam    9  1000.0\n",
       "#Ankara      57   313.0\n",
       "#Athens      99  1000.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agg num_tweets column\n",
    "sc100_features = pd.concat([sc100_features, smartcities100['query'].value_counts()], axis=1)\n",
    "sc100_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>query</th>\n",
       "      <th>total_tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#AbuDhabi</th>\n",
       "      <td>42</td>\n",
       "      <td>832.0</td>\n",
       "      <td>161350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Abuja</th>\n",
       "      <td>107</td>\n",
       "      <td>828.0</td>\n",
       "      <td>173587.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Amsterdam</th>\n",
       "      <td>9</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>180311.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Ankara</th>\n",
       "      <td>57</td>\n",
       "      <td>313.0</td>\n",
       "      <td>58158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Athens</th>\n",
       "      <td>99</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>186752.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            min   query  total_tweet_len\n",
       "#AbuDhabi    42   832.0         161350.0\n",
       "#Abuja      107   828.0         173587.0\n",
       "#Amsterdam    9  1000.0         180311.0\n",
       "#Ankara      57   313.0          58158.0\n",
       "#Athens      99  1000.0         186752.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agg total tweet length by city\n",
    "sc100_features['total_tweet_len'] = smartcities100.groupby('query')['tweet_len'].sum()\n",
    "sc100_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average tweet length\n",
    "sc100_features['avg_tweet_len'] = sc100_features['total_tweet_len']/sc100_features['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg total number of words \n",
    "sc100_features['total_num_words'] = smartcities100.groupby('query')['num_words'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average number of words\n",
    "sc100_features['avg_num_words'] = sc100_features['total_num_words']/sc100_features['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg total number of stopwords \n",
    "sc100_features['total_stopwords'] = smartcities100.groupby('query')['stopwords'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average number of stopwords\n",
    "sc100_features['avg_stopwords'] = sc100_features['total_stopwords']/sc100_features['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg total number of sentences \n",
    "sc100_features['total_num_sentences'] = smartcities100.groupby('query')['num_sentences'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average number of sentences\n",
    "sc100_features['avg_num_sentences'] = sc100_features['total_num_sentences']/sc100_features['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg total average word length\n",
    "sc100_features['total_avg_word_len'] = smartcities100.groupby('query')['avg_word_len'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average of average word length\n",
    "sc100_features['avg_avg_word_len'] = sc100_features['total_avg_word_len']/sc100_features['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg total number of punctuation\n",
    "sc100_features['total_punctuation'] = smartcities100.groupby('query')['punctuation'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average number of punctuation\n",
    "sc100_features['avg_punctuation'] = sc100_features['total_punctuation']/sc100_features['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg total number of hashtags\n",
    "sc100_features['total_hashtags'] = smartcities100.groupby('query')['hashtags'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average number of hashtags\n",
    "sc100_features['avg_hashtags'] = sc100_features['total_hashtags']/sc100_features['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg total number of numerics\n",
    "sc100_features['total_numerics'] = smartcities100.groupby('query')['numerics'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average number of numerics\n",
    "sc100_features['avg_numerics'] = sc100_features['total_numerics']/sc100_features['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg total number of uppercase words\n",
    "sc100_features['total_upper'] = smartcities100.groupby('query')['upper'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average number of uppercase words\n",
    "sc100_features['avg_upper'] = sc100_features['total_upper']/sc100_features['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg total sentiment scores\n",
    "sc100_features['neg'] = smartcities100.groupby('query')['neg'].sum()\n",
    "sc100_features['neu'] = smartcities100.groupby('query')['neu'].sum()\n",
    "sc100_features['pos'] = smartcities100.groupby('query')['pos'].sum()\n",
    "sc100_features['compound'] = smartcities100.groupby('query')['compound'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average sentiment scores\n",
    "sc100_features['avg_neg'] = sc100_features['neg']/sc100_features['query']\n",
    "sc100_features['avg_neu'] = sc100_features['neu']/sc100_features['query']\n",
    "sc100_features['avg_pos'] = sc100_features['pos']/sc100_features['query']\n",
    "sc100_features['avg_compound'] = sc100_features['compound']/sc100_features['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg polarity and subjectivity scores\n",
    "sc100_features['polarity'] = smartcities100.groupby('query')['polarity_tweet'].sum()\n",
    "sc100_features['subjectivity'] = smartcities100.groupby('query')['subjectivity_tweet'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average sentiment scores\n",
    "sc100_features['avg_polarity'] = sc100_features['polarity']/sc100_features['query']\n",
    "sc100_features['avg_subjectivity'] = sc100_features['subjectivity']/sc100_features['query']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# agg total likes per city\n",
    "sc100_features['total_likes'] = smartcities100.groupby('query')['likes'].sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# calculate average likes on each tweets per city\n",
    "sc100_features['avg_likes'] = sc100_features['total_likes']/sc100_features['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>query</th>\n",
       "      <th>total_tweet_len</th>\n",
       "      <th>avg_tweet_len</th>\n",
       "      <th>total_num_words</th>\n",
       "      <th>avg_num_words</th>\n",
       "      <th>total_stopwords</th>\n",
       "      <th>avg_stopwords</th>\n",
       "      <th>total_num_sentences</th>\n",
       "      <th>avg_num_sentences</th>\n",
       "      <th>total_avg_word_len</th>\n",
       "      <th>avg_avg_word_len</th>\n",
       "      <th>total_punctuation</th>\n",
       "      <th>avg_punctuation</th>\n",
       "      <th>total_hashtags</th>\n",
       "      <th>avg_hashtags</th>\n",
       "      <th>total_numerics</th>\n",
       "      <th>avg_numerics</th>\n",
       "      <th>total_upper</th>\n",
       "      <th>avg_upper</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>avg_neg</th>\n",
       "      <th>avg_neu</th>\n",
       "      <th>avg_pos</th>\n",
       "      <th>avg_compound</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>avg_polarity</th>\n",
       "      <th>avg_subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#AbuDhabi</th>\n",
       "      <td>42</td>\n",
       "      <td>832.0</td>\n",
       "      <td>161350.0</td>\n",
       "      <td>193.930288</td>\n",
       "      <td>20802.0</td>\n",
       "      <td>25.002404</td>\n",
       "      <td>4821.0</td>\n",
       "      <td>5.794471</td>\n",
       "      <td>1194.0</td>\n",
       "      <td>1.435096</td>\n",
       "      <td>5887.253316</td>\n",
       "      <td>7.076026</td>\n",
       "      <td>11662.0</td>\n",
       "      <td>14.016827</td>\n",
       "      <td>3829.0</td>\n",
       "      <td>4.602163</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.278846</td>\n",
       "      <td>908.0</td>\n",
       "      <td>1.091346</td>\n",
       "      <td>18.248</td>\n",
       "      <td>715.630</td>\n",
       "      <td>98.123</td>\n",
       "      <td>249.5025</td>\n",
       "      <td>0.021933</td>\n",
       "      <td>0.860132</td>\n",
       "      <td>0.117936</td>\n",
       "      <td>0.299883</td>\n",
       "      <td>158.079259</td>\n",
       "      <td>290.438874</td>\n",
       "      <td>0.189999</td>\n",
       "      <td>0.349085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Abuja</th>\n",
       "      <td>107</td>\n",
       "      <td>828.0</td>\n",
       "      <td>173587.0</td>\n",
       "      <td>209.646135</td>\n",
       "      <td>22391.0</td>\n",
       "      <td>27.042271</td>\n",
       "      <td>4621.0</td>\n",
       "      <td>5.580918</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>1.437198</td>\n",
       "      <td>5858.506206</td>\n",
       "      <td>7.075491</td>\n",
       "      <td>12726.0</td>\n",
       "      <td>15.369565</td>\n",
       "      <td>4792.0</td>\n",
       "      <td>5.787440</td>\n",
       "      <td>335.0</td>\n",
       "      <td>0.404589</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>1.263285</td>\n",
       "      <td>15.064</td>\n",
       "      <td>709.099</td>\n",
       "      <td>103.828</td>\n",
       "      <td>295.2426</td>\n",
       "      <td>0.018193</td>\n",
       "      <td>0.856400</td>\n",
       "      <td>0.125396</td>\n",
       "      <td>0.356573</td>\n",
       "      <td>154.819778</td>\n",
       "      <td>302.964433</td>\n",
       "      <td>0.186980</td>\n",
       "      <td>0.365899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Amsterdam</th>\n",
       "      <td>9</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>180311.0</td>\n",
       "      <td>180.311000</td>\n",
       "      <td>22627.0</td>\n",
       "      <td>22.627000</td>\n",
       "      <td>5220.0</td>\n",
       "      <td>5.220000</td>\n",
       "      <td>1520.0</td>\n",
       "      <td>1.520000</td>\n",
       "      <td>7579.218712</td>\n",
       "      <td>7.579219</td>\n",
       "      <td>14554.0</td>\n",
       "      <td>14.554000</td>\n",
       "      <td>5239.0</td>\n",
       "      <td>5.239000</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>663.0</td>\n",
       "      <td>0.663000</td>\n",
       "      <td>24.567</td>\n",
       "      <td>875.009</td>\n",
       "      <td>100.439</td>\n",
       "      <td>250.7977</td>\n",
       "      <td>0.024567</td>\n",
       "      <td>0.875009</td>\n",
       "      <td>0.100439</td>\n",
       "      <td>0.250798</td>\n",
       "      <td>152.441578</td>\n",
       "      <td>316.409962</td>\n",
       "      <td>0.152442</td>\n",
       "      <td>0.316410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Ankara</th>\n",
       "      <td>57</td>\n",
       "      <td>313.0</td>\n",
       "      <td>58158.0</td>\n",
       "      <td>185.808307</td>\n",
       "      <td>7194.0</td>\n",
       "      <td>22.984026</td>\n",
       "      <td>1244.0</td>\n",
       "      <td>3.974441</td>\n",
       "      <td>409.0</td>\n",
       "      <td>1.306709</td>\n",
       "      <td>2350.814230</td>\n",
       "      <td>7.510589</td>\n",
       "      <td>4616.0</td>\n",
       "      <td>14.747604</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>5.623003</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.338658</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.840256</td>\n",
       "      <td>12.420</td>\n",
       "      <td>271.031</td>\n",
       "      <td>29.554</td>\n",
       "      <td>60.1001</td>\n",
       "      <td>0.039681</td>\n",
       "      <td>0.865914</td>\n",
       "      <td>0.094422</td>\n",
       "      <td>0.192013</td>\n",
       "      <td>40.076816</td>\n",
       "      <td>75.456690</td>\n",
       "      <td>0.128041</td>\n",
       "      <td>0.241076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Athens</th>\n",
       "      <td>99</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>186752.0</td>\n",
       "      <td>186.752000</td>\n",
       "      <td>24268.0</td>\n",
       "      <td>24.268000</td>\n",
       "      <td>5405.0</td>\n",
       "      <td>5.405000</td>\n",
       "      <td>1503.0</td>\n",
       "      <td>1.503000</td>\n",
       "      <td>7170.591520</td>\n",
       "      <td>7.170592</td>\n",
       "      <td>15201.0</td>\n",
       "      <td>15.201000</td>\n",
       "      <td>5442.0</td>\n",
       "      <td>5.442000</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>865.0</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>42.164</td>\n",
       "      <td>865.646</td>\n",
       "      <td>92.190</td>\n",
       "      <td>176.3919</td>\n",
       "      <td>0.042164</td>\n",
       "      <td>0.865646</td>\n",
       "      <td>0.092190</td>\n",
       "      <td>0.176392</td>\n",
       "      <td>126.461888</td>\n",
       "      <td>320.792854</td>\n",
       "      <td>0.126462</td>\n",
       "      <td>0.320793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            min   query  total_tweet_len  avg_tweet_len  total_num_words  \\\n",
       "#AbuDhabi    42   832.0         161350.0     193.930288          20802.0   \n",
       "#Abuja      107   828.0         173587.0     209.646135          22391.0   \n",
       "#Amsterdam    9  1000.0         180311.0     180.311000          22627.0   \n",
       "#Ankara      57   313.0          58158.0     185.808307           7194.0   \n",
       "#Athens      99  1000.0         186752.0     186.752000          24268.0   \n",
       "\n",
       "            avg_num_words  total_stopwords  avg_stopwords  \\\n",
       "#AbuDhabi       25.002404           4821.0       5.794471   \n",
       "#Abuja          27.042271           4621.0       5.580918   \n",
       "#Amsterdam      22.627000           5220.0       5.220000   \n",
       "#Ankara         22.984026           1244.0       3.974441   \n",
       "#Athens         24.268000           5405.0       5.405000   \n",
       "\n",
       "            total_num_sentences  avg_num_sentences  total_avg_word_len  \\\n",
       "#AbuDhabi                1194.0           1.435096         5887.253316   \n",
       "#Abuja                   1190.0           1.437198         5858.506206   \n",
       "#Amsterdam               1520.0           1.520000         7579.218712   \n",
       "#Ankara                   409.0           1.306709         2350.814230   \n",
       "#Athens                  1503.0           1.503000         7170.591520   \n",
       "\n",
       "            avg_avg_word_len  total_punctuation  avg_punctuation  \\\n",
       "#AbuDhabi           7.076026            11662.0        14.016827   \n",
       "#Abuja              7.075491            12726.0        15.369565   \n",
       "#Amsterdam          7.579219            14554.0        14.554000   \n",
       "#Ankara             7.510589             4616.0        14.747604   \n",
       "#Athens             7.170592            15201.0        15.201000   \n",
       "\n",
       "            total_hashtags  avg_hashtags  total_numerics  avg_numerics  \\\n",
       "#AbuDhabi           3829.0      4.602163           232.0      0.278846   \n",
       "#Abuja              4792.0      5.787440           335.0      0.404589   \n",
       "#Amsterdam          5239.0      5.239000           153.0      0.153000   \n",
       "#Ankara             1760.0      5.623003           106.0      0.338658   \n",
       "#Athens             5442.0      5.442000           226.0      0.226000   \n",
       "\n",
       "            total_upper  avg_upper     neg      neu      pos  compound  \\\n",
       "#AbuDhabi         908.0   1.091346  18.248  715.630   98.123  249.5025   \n",
       "#Abuja           1046.0   1.263285  15.064  709.099  103.828  295.2426   \n",
       "#Amsterdam        663.0   0.663000  24.567  875.009  100.439  250.7977   \n",
       "#Ankara           263.0   0.840256  12.420  271.031   29.554   60.1001   \n",
       "#Athens           865.0   0.865000  42.164  865.646   92.190  176.3919   \n",
       "\n",
       "             avg_neg   avg_neu   avg_pos  avg_compound    polarity  \\\n",
       "#AbuDhabi   0.021933  0.860132  0.117936      0.299883  158.079259   \n",
       "#Abuja      0.018193  0.856400  0.125396      0.356573  154.819778   \n",
       "#Amsterdam  0.024567  0.875009  0.100439      0.250798  152.441578   \n",
       "#Ankara     0.039681  0.865914  0.094422      0.192013   40.076816   \n",
       "#Athens     0.042164  0.865646  0.092190      0.176392  126.461888   \n",
       "\n",
       "            subjectivity  avg_polarity  avg_subjectivity  \n",
       "#AbuDhabi     290.438874      0.189999          0.349085  \n",
       "#Abuja        302.964433      0.186980          0.365899  \n",
       "#Amsterdam    316.409962      0.152442          0.316410  \n",
       "#Ankara        75.456690      0.128041          0.241076  \n",
       "#Athens       320.792854      0.126462          0.320793  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc100_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc100_features = sc100_features.drop(columns=['total_tweet_len','total_num_words','total_stopwords','total_num_sentences','total_avg_word_len','total_punctuation','total_hashtags','total_numerics','total_upper','neg','neu','pos','compound','polarity','subjectivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column with lists of merged tweets grouped by city\n",
    "sc100_features = pd.concat([sc100_features, smartcities100.groupby('query')['content'].apply(lambda x: x.tolist())], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the merged tweets lists into strings\n",
    "sc100_features['content'] = sc100_features['content'].apply(lambda x: str(x).strip('[]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of words for each city\n",
    "sc100_features['words'] = sc100_features['content'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of stopwords for each city\n",
    "sc100_features['stopwords'] = sc100_features['content'].apply(lambda x: len([x for x in x.split() if x in stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of punctuation for each city\n",
    "sc100_features['punctuation'] = sc100_features['content'].apply(lambda x: count_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of hashtags for each city\n",
    "sc100_features['hashtags'] = sc100_features['content'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of numerics for each city\n",
    "sc100_features['numerics'] = sc100_features['content'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of uppercase words for each city\n",
    "sc100_features['upper'] = sc100_features['content'].apply(lambda x: len([x for x in x.split() if x.isupper()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning tweets merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column to lower characters of tweets merged\n",
    "sc100_features['clean_text'] = sc100_features['content'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert emojis into words\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "\n",
    "sc100_features['clean_text'] = sc100_features['clean_text'].apply(lambda x: convert_emojis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert emoticons into words\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "sc100_features['clean_text'] = sc100_features['clean_text'].apply(lambda x: convert_emoticons(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "sc100_features['clean_text'] = sc100_features['clean_text'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove symbols\n",
    "sc100_features['clean_text'] = sc100_features['clean_text'].apply(lambda x: re.sub(r'.,;?!/\\&*$€£`§\"+-%|:_’`()','', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "sc100_features['clean_text'] = sc100_features['clean_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls\n",
    "def remove_url(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "sc100_features['clean_text'] = sc100_features['clean_text'].apply(lambda x: remove_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove htmls\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "sc100_features['clean_text'] = sc100_features['clean_text'].apply(lambda x: remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove # \n",
    "import re\n",
    "sc100_features['clean_text'] = sc100_features['clean_text'].apply(lambda x: re.sub(r'#',' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove @mentions\n",
    "sc100_features['clean_text'] = sc100_features['clean_text'].apply(lambda x: re.sub(r'@[A-Za-z0-9]+','', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove digits\n",
    "sc100_features['clean_text'] = sc100_features['clean_text'].apply(lambda x: re.sub(r'[0-9]+','', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand contractions \n",
    "contractions = contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "def cont_to_exp(x):\n",
    "    if type(x) is str:\n",
    "        x = x.replace('\\\\','')\n",
    "        for key in contractions:\n",
    "            value = contractions[key]\n",
    "            x = x.replace(key,value)\n",
    "        return x\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "sc100_features['clean_text'] = sc100_features['clean_text'].apply(lambda x: cont_to_exp(x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# too long, to fix : spell correction\n",
    "sc100_features['clean_text'] = sc100_features['clean_text'].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of clean words\n",
    "sc100_features['num_clean_words'] = sc100_features['clean_text'].apply(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of characters into clean text (incl. spaces)\n",
    "sc100_features['num_clean_characters'] = sc100_features['clean_text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average word length for clean words\n",
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/(len(words)+0.000001))\n",
    "\n",
    "sc100_features['avg_clean_word_len'] = sc100_features['clean_text'].apply(lambda x: avg_word(x)).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis from clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment polarity and subjectivity with textblob\n",
    "sc100_features['polarity_clean'] = sc100_features['clean_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "sc100_features['subjectivity_clean'] = sc100_features['clean_text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize clean text into a dedicated column\n",
    "sc100_features['tokens'] = sc100_features['clean_text'].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove useless tokens\n",
    "useless = [\"com\",\"#\",'&','!',\"gt\",\"'re\",\"\\n\",\"amp\",\"als\",\"'s\",\"als\", \"etc\",'\\n', \"\\\\n\",'\\\\n\\\\n',']','[','.','...','“',',',\"'\",\"''\",'th','…','“',':']\n",
    "\n",
    "sc100_features['tokens'] = sc100_features['tokens'].apply(lambda x: [item for item in x if item not in useless])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove single letter words\n",
    "sc100_features['tokens'] = sc100_features['tokens'].apply(lambda x: [item for item in x if len(item)>1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part of speech tagging on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc100_features['pos_tag'] = sc100_features['tokens'].apply(lambda x: nltk.pos_tag(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## most frequent 100 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count most freq 100 words for each city\n",
    "sc100_features['most_freq100'] = sc100_features['tokens'].apply(lambda x: FreqDist(x).most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use algorythms to create some more variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "sc100_features['count_vec'] = sc100_features['tokens'].apply(lambda x: count_vect.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "sc100_features['tfidf'] = sc100_features['count_vec'].apply(lambda x: tfidf_transformer.fit_transform(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### those two features will be useful to train a classifier prediction same as multinomialNB\n",
    "- clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
    "- clf2 = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None))\n",
    "\n",
    "#### parameter tuning can be done using grid search\n",
    "- parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}, \n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN row and reset index\n",
    "sc100_features = sc100_features.reset_index().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>min</th>\n",
       "      <th>query</th>\n",
       "      <th>avg_tweet_len</th>\n",
       "      <th>avg_num_words</th>\n",
       "      <th>avg_stopwords</th>\n",
       "      <th>avg_num_sentences</th>\n",
       "      <th>avg_avg_word_len</th>\n",
       "      <th>avg_punctuation</th>\n",
       "      <th>avg_hashtags</th>\n",
       "      <th>avg_numerics</th>\n",
       "      <th>avg_upper</th>\n",
       "      <th>avg_neg</th>\n",
       "      <th>avg_neu</th>\n",
       "      <th>avg_pos</th>\n",
       "      <th>avg_compound</th>\n",
       "      <th>avg_polarity</th>\n",
       "      <th>avg_subjectivity</th>\n",
       "      <th>content</th>\n",
       "      <th>words</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>punctuation</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>num_clean_words</th>\n",
       "      <th>num_clean_characters</th>\n",
       "      <th>avg_clean_word_len</th>\n",
       "      <th>polarity_clean</th>\n",
       "      <th>subjectivity_clean</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>most_freq100</th>\n",
       "      <th>count_vec</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#AbuDhabi</td>\n",
       "      <td>42</td>\n",
       "      <td>832.0</td>\n",
       "      <td>193.930288</td>\n",
       "      <td>25.002404</td>\n",
       "      <td>5.794471</td>\n",
       "      <td>1.435096</td>\n",
       "      <td>7.076026</td>\n",
       "      <td>14.016827</td>\n",
       "      <td>4.602163</td>\n",
       "      <td>0.278846</td>\n",
       "      <td>1.091346</td>\n",
       "      <td>0.021933</td>\n",
       "      <td>0.860132</td>\n",
       "      <td>0.117936</td>\n",
       "      <td>0.299883</td>\n",
       "      <td>0.189999</td>\n",
       "      <td>0.349085</td>\n",
       "      <td>'A Delegation from  @aau_ae visited @BurjeelMe...</td>\n",
       "      <td>166602</td>\n",
       "      <td>4811</td>\n",
       "      <td>15703</td>\n",
       "      <td>3268</td>\n",
       "      <td>202</td>\n",
       "      <td>760</td>\n",
       "      <td>delegation aau_ae visited burjeelmedicity one ...</td>\n",
       "      <td>14484</td>\n",
       "      <td>173030</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.242483</td>\n",
       "      <td>0.471562</td>\n",
       "      <td>[delegation, aau_ae, visited, burjeelmedicity,...</td>\n",
       "      <td>[(delegation, NN), (aau_ae, NN), (visited, VBD...</td>\n",
       "      <td>[(abudhabi, 677), (uae, 224), (abu, 156), (dub...</td>\n",
       "      <td>(0, 1235)\\t1\\n  (1, 2)\\t1\\n  (2, 5825)\\t1\\n ...</td>\n",
       "      <td>(0, 1235)\\t1.0\\n  (1, 2)\\t1.0\\n  (2, 5825)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Abuja</td>\n",
       "      <td>107</td>\n",
       "      <td>828.0</td>\n",
       "      <td>209.646135</td>\n",
       "      <td>27.042271</td>\n",
       "      <td>5.580918</td>\n",
       "      <td>1.437198</td>\n",
       "      <td>7.075491</td>\n",
       "      <td>15.369565</td>\n",
       "      <td>5.787440</td>\n",
       "      <td>0.404589</td>\n",
       "      <td>1.263285</td>\n",
       "      <td>0.018193</td>\n",
       "      <td>0.856400</td>\n",
       "      <td>0.125396</td>\n",
       "      <td>0.356573</td>\n",
       "      <td>0.186980</td>\n",
       "      <td>0.365899</td>\n",
       "      <td>'Brother #Abuja https://t.co/6J3yOIP8Jm', 'Our...</td>\n",
       "      <td>180295</td>\n",
       "      <td>4561</td>\n",
       "      <td>18553</td>\n",
       "      <td>3862</td>\n",
       "      <td>232</td>\n",
       "      <td>766</td>\n",
       "      <td>brother abuja httpsSkeptical_annoyed_undecided...</td>\n",
       "      <td>15083</td>\n",
       "      <td>182674</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.249437</td>\n",
       "      <td>0.498789</td>\n",
       "      <td>[brother, abuja, httpsSkeptical_annoyed_undeci...</td>\n",
       "      <td>[(brother, NN), (abuja, NN), (httpsSkeptical_a...</td>\n",
       "      <td>[(abuja, 635), (lagos, 163), (abujatwittercomm...</td>\n",
       "      <td>(0, 732)\\t1\\n  (1, 24)\\t1\\n  (2, 2539)\\t1\\n ...</td>\n",
       "      <td>(0, 732)\\t1.0\\n  (1, 24)\\t1.0\\n  (2, 2539)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Amsterdam</td>\n",
       "      <td>9</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>180.311000</td>\n",
       "      <td>22.627000</td>\n",
       "      <td>5.220000</td>\n",
       "      <td>1.520000</td>\n",
       "      <td>7.579219</td>\n",
       "      <td>14.554000</td>\n",
       "      <td>5.239000</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>0.663000</td>\n",
       "      <td>0.024567</td>\n",
       "      <td>0.875009</td>\n",
       "      <td>0.100439</td>\n",
       "      <td>0.250798</td>\n",
       "      <td>0.152442</td>\n",
       "      <td>0.316410</td>\n",
       "      <td>'Herfst in #Amsterdam 🍂🍃🍁 https://t.co/F0VKRmq...</td>\n",
       "      <td>185912</td>\n",
       "      <td>5200</td>\n",
       "      <td>18994</td>\n",
       "      <td>4740</td>\n",
       "      <td>134</td>\n",
       "      <td>563</td>\n",
       "      <td>herfst amsterdam fallen_leafleaf_fluttering_in...</td>\n",
       "      <td>15579</td>\n",
       "      <td>197584</td>\n",
       "      <td>11.9</td>\n",
       "      <td>0.218700</td>\n",
       "      <td>0.476012</td>\n",
       "      <td>[herfst, amsterdam, fallen_leafleaf_fluttering...</td>\n",
       "      <td>[(herfst, NN), (amsterdam, NN), (fallen_leafle...</td>\n",
       "      <td>[(amsterdam, 993), (netherlands, 80), (day, 64...</td>\n",
       "      <td>(0, 1939)\\t1\\n  (1, 171)\\t1\\n  (2, 1517)\\t1\\...</td>\n",
       "      <td>(0, 1939)\\t1.0\\n  (1, 171)\\t1.0\\n  (2, 1517)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Ankara</td>\n",
       "      <td>57</td>\n",
       "      <td>313.0</td>\n",
       "      <td>185.808307</td>\n",
       "      <td>22.984026</td>\n",
       "      <td>3.974441</td>\n",
       "      <td>1.306709</td>\n",
       "      <td>7.510589</td>\n",
       "      <td>14.747604</td>\n",
       "      <td>5.623003</td>\n",
       "      <td>0.338658</td>\n",
       "      <td>0.840256</td>\n",
       "      <td>0.039681</td>\n",
       "      <td>0.865914</td>\n",
       "      <td>0.094422</td>\n",
       "      <td>0.192013</td>\n",
       "      <td>0.128041</td>\n",
       "      <td>0.241076</td>\n",
       "      <td>'I had a great and very joyful flight today ✈️...</td>\n",
       "      <td>60230</td>\n",
       "      <td>1241</td>\n",
       "      <td>6328</td>\n",
       "      <td>1426</td>\n",
       "      <td>77</td>\n",
       "      <td>224</td>\n",
       "      <td>great joyful flight today airplane nto ankara ...</td>\n",
       "      <td>5302</td>\n",
       "      <td>66000</td>\n",
       "      <td>11.7</td>\n",
       "      <td>0.164460</td>\n",
       "      <td>0.419755</td>\n",
       "      <td>[great, joyful, flight, today, airplane, nto, ...</td>\n",
       "      <td>[(great, JJ), (joyful, JJ), (flight, NN), (tod...</td>\n",
       "      <td>[(ankara, 251), (turkey, 77), (turkish, 26), (...</td>\n",
       "      <td>(0, 1059)\\t1\\n  (1, 1592)\\t1\\n  (2, 952)\\t1\\...</td>\n",
       "      <td>(0, 1059)\\t1.0\\n  (1, 1592)\\t1.0\\n  (2, 952)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Athens</td>\n",
       "      <td>99</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>186.752000</td>\n",
       "      <td>24.268000</td>\n",
       "      <td>5.405000</td>\n",
       "      <td>1.503000</td>\n",
       "      <td>7.170592</td>\n",
       "      <td>15.201000</td>\n",
       "      <td>5.442000</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.042164</td>\n",
       "      <td>0.865646</td>\n",
       "      <td>0.092190</td>\n",
       "      <td>0.176392</td>\n",
       "      <td>0.126462</td>\n",
       "      <td>0.320793</td>\n",
       "      <td>'Georgia GA #GARunoffs\\n#Sparta #Athens #Ciara...</td>\n",
       "      <td>192839</td>\n",
       "      <td>5370</td>\n",
       "      <td>20229</td>\n",
       "      <td>4795</td>\n",
       "      <td>180</td>\n",
       "      <td>766</td>\n",
       "      <td>georgia ga garunoffsnsparta athens ciara lukes...</td>\n",
       "      <td>16782</td>\n",
       "      <td>204722</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.173722</td>\n",
       "      <td>0.448072</td>\n",
       "      <td>[georgia, ga, garunoffsnsparta, athens, ciara,...</td>\n",
       "      <td>[(georgia, NN), (ga, NN), (garunoffsnsparta, N...</td>\n",
       "      <td>[(athens, 867), (greece, 322), (greek, 74), (a...</td>\n",
       "      <td>(0, 1653)\\t1\\n  (1, 1613)\\t1\\n  (2, 1628)\\t1...</td>\n",
       "      <td>(0, 1653)\\t1.0\\n  (1, 1613)\\t1.0\\n  (2, 1628...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  min   query  avg_tweet_len  avg_num_words  avg_stopwords  \\\n",
       "0   #AbuDhabi   42   832.0     193.930288      25.002404       5.794471   \n",
       "1      #Abuja  107   828.0     209.646135      27.042271       5.580918   \n",
       "2  #Amsterdam    9  1000.0     180.311000      22.627000       5.220000   \n",
       "3     #Ankara   57   313.0     185.808307      22.984026       3.974441   \n",
       "4     #Athens   99  1000.0     186.752000      24.268000       5.405000   \n",
       "\n",
       "   avg_num_sentences  avg_avg_word_len  avg_punctuation  avg_hashtags  \\\n",
       "0           1.435096          7.076026        14.016827      4.602163   \n",
       "1           1.437198          7.075491        15.369565      5.787440   \n",
       "2           1.520000          7.579219        14.554000      5.239000   \n",
       "3           1.306709          7.510589        14.747604      5.623003   \n",
       "4           1.503000          7.170592        15.201000      5.442000   \n",
       "\n",
       "   avg_numerics  avg_upper   avg_neg   avg_neu   avg_pos  avg_compound  \\\n",
       "0      0.278846   1.091346  0.021933  0.860132  0.117936      0.299883   \n",
       "1      0.404589   1.263285  0.018193  0.856400  0.125396      0.356573   \n",
       "2      0.153000   0.663000  0.024567  0.875009  0.100439      0.250798   \n",
       "3      0.338658   0.840256  0.039681  0.865914  0.094422      0.192013   \n",
       "4      0.226000   0.865000  0.042164  0.865646  0.092190      0.176392   \n",
       "\n",
       "   avg_polarity  avg_subjectivity  \\\n",
       "0      0.189999          0.349085   \n",
       "1      0.186980          0.365899   \n",
       "2      0.152442          0.316410   \n",
       "3      0.128041          0.241076   \n",
       "4      0.126462          0.320793   \n",
       "\n",
       "                                             content   words  stopwords  \\\n",
       "0  'A Delegation from  @aau_ae visited @BurjeelMe...  166602       4811   \n",
       "1  'Brother #Abuja https://t.co/6J3yOIP8Jm', 'Our...  180295       4561   \n",
       "2  'Herfst in #Amsterdam 🍂🍃🍁 https://t.co/F0VKRmq...  185912       5200   \n",
       "3  'I had a great and very joyful flight today ✈️...   60230       1241   \n",
       "4  'Georgia GA #GARunoffs\\n#Sparta #Athens #Ciara...  192839       5370   \n",
       "\n",
       "   punctuation  hashtags  numerics  upper  \\\n",
       "0        15703      3268       202    760   \n",
       "1        18553      3862       232    766   \n",
       "2        18994      4740       134    563   \n",
       "3         6328      1426        77    224   \n",
       "4        20229      4795       180    766   \n",
       "\n",
       "                                          clean_text  num_clean_words  \\\n",
       "0  delegation aau_ae visited burjeelmedicity one ...            14484   \n",
       "1  brother abuja httpsSkeptical_annoyed_undecided...            15083   \n",
       "2  herfst amsterdam fallen_leafleaf_fluttering_in...            15579   \n",
       "3  great joyful flight today airplane nto ankara ...             5302   \n",
       "4  georgia ga garunoffsnsparta athens ciara lukes...            16782   \n",
       "\n",
       "   num_clean_characters  avg_clean_word_len  polarity_clean  \\\n",
       "0                173030                11.2        0.242483   \n",
       "1                182674                11.4        0.249437   \n",
       "2                197584                11.9        0.218700   \n",
       "3                 66000                11.7        0.164460   \n",
       "4                204722                11.5        0.173722   \n",
       "\n",
       "   subjectivity_clean                                             tokens  \\\n",
       "0            0.471562  [delegation, aau_ae, visited, burjeelmedicity,...   \n",
       "1            0.498789  [brother, abuja, httpsSkeptical_annoyed_undeci...   \n",
       "2            0.476012  [herfst, amsterdam, fallen_leafleaf_fluttering...   \n",
       "3            0.419755  [great, joyful, flight, today, airplane, nto, ...   \n",
       "4            0.448072  [georgia, ga, garunoffsnsparta, athens, ciara,...   \n",
       "\n",
       "                                             pos_tag  \\\n",
       "0  [(delegation, NN), (aau_ae, NN), (visited, VBD...   \n",
       "1  [(brother, NN), (abuja, NN), (httpsSkeptical_a...   \n",
       "2  [(herfst, NN), (amsterdam, NN), (fallen_leafle...   \n",
       "3  [(great, JJ), (joyful, JJ), (flight, NN), (tod...   \n",
       "4  [(georgia, NN), (ga, NN), (garunoffsnsparta, N...   \n",
       "\n",
       "                                        most_freq100  \\\n",
       "0  [(abudhabi, 677), (uae, 224), (abu, 156), (dub...   \n",
       "1  [(abuja, 635), (lagos, 163), (abujatwittercomm...   \n",
       "2  [(amsterdam, 993), (netherlands, 80), (day, 64...   \n",
       "3  [(ankara, 251), (turkey, 77), (turkish, 26), (...   \n",
       "4  [(athens, 867), (greece, 322), (greek, 74), (a...   \n",
       "\n",
       "                                           count_vec  \\\n",
       "0    (0, 1235)\\t1\\n  (1, 2)\\t1\\n  (2, 5825)\\t1\\n ...   \n",
       "1    (0, 732)\\t1\\n  (1, 24)\\t1\\n  (2, 2539)\\t1\\n ...   \n",
       "2    (0, 1939)\\t1\\n  (1, 171)\\t1\\n  (2, 1517)\\t1\\...   \n",
       "3    (0, 1059)\\t1\\n  (1, 1592)\\t1\\n  (2, 952)\\t1\\...   \n",
       "4    (0, 1653)\\t1\\n  (1, 1613)\\t1\\n  (2, 1628)\\t1...   \n",
       "\n",
       "                                               tfidf  \n",
       "0    (0, 1235)\\t1.0\\n  (1, 2)\\t1.0\\n  (2, 5825)\\t...  \n",
       "1    (0, 732)\\t1.0\\n  (1, 24)\\t1.0\\n  (2, 2539)\\t...  \n",
       "2    (0, 1939)\\t1.0\\n  (1, 171)\\t1.0\\n  (2, 1517)...  \n",
       "3    (0, 1059)\\t1.0\\n  (1, 1592)\\t1.0\\n  (2, 952)...  \n",
       "4    (0, 1653)\\t1.0\\n  (1, 1613)\\t1.0\\n  (2, 1628...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check my dataframe\n",
    "sc100_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_id</th>\n",
       "      <th>rank2020</th>\n",
       "      <th>num_tweets</th>\n",
       "      <th>avg_tweet_len</th>\n",
       "      <th>avg_num_words</th>\n",
       "      <th>avg_stopwords</th>\n",
       "      <th>avg_num_sentences</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>avg_punctuation</th>\n",
       "      <th>avg_hashtags</th>\n",
       "      <th>avg_numerics</th>\n",
       "      <th>avg_upper</th>\n",
       "      <th>avg_neg</th>\n",
       "      <th>avg_neu</th>\n",
       "      <th>avg_pos</th>\n",
       "      <th>avg_compound</th>\n",
       "      <th>avg_polarity</th>\n",
       "      <th>avg_subjectivity</th>\n",
       "      <th>tweets_merged</th>\n",
       "      <th>words_total</th>\n",
       "      <th>stopwords_total</th>\n",
       "      <th>punctuation_total</th>\n",
       "      <th>hashtags_total</th>\n",
       "      <th>numerics_total</th>\n",
       "      <th>upper_total</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_words_total</th>\n",
       "      <th>clean_char_total</th>\n",
       "      <th>avg_clean_word_len</th>\n",
       "      <th>polarity_clean</th>\n",
       "      <th>subjectivity_clean</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>most_freq100</th>\n",
       "      <th>count_vec</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#AbuDhabi</td>\n",
       "      <td>42</td>\n",
       "      <td>832.0</td>\n",
       "      <td>193.930288</td>\n",
       "      <td>25.002404</td>\n",
       "      <td>5.794471</td>\n",
       "      <td>1.435096</td>\n",
       "      <td>7.076026</td>\n",
       "      <td>14.016827</td>\n",
       "      <td>4.602163</td>\n",
       "      <td>0.278846</td>\n",
       "      <td>1.091346</td>\n",
       "      <td>0.021933</td>\n",
       "      <td>0.860132</td>\n",
       "      <td>0.117936</td>\n",
       "      <td>0.299883</td>\n",
       "      <td>0.189999</td>\n",
       "      <td>0.349085</td>\n",
       "      <td>'A Delegation from  @aau_ae visited @BurjeelMe...</td>\n",
       "      <td>166602</td>\n",
       "      <td>4811</td>\n",
       "      <td>15703</td>\n",
       "      <td>3268</td>\n",
       "      <td>202</td>\n",
       "      <td>760</td>\n",
       "      <td>delegation aau_ae visited burjeelmedicity one ...</td>\n",
       "      <td>14484</td>\n",
       "      <td>173030</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.242483</td>\n",
       "      <td>0.471562</td>\n",
       "      <td>[delegation, aau_ae, visited, burjeelmedicity,...</td>\n",
       "      <td>[(delegation, NN), (aau_ae, NN), (visited, VBD...</td>\n",
       "      <td>[(abudhabi, 677), (uae, 224), (abu, 156), (dub...</td>\n",
       "      <td>(0, 1235)\\t1\\n  (1, 2)\\t1\\n  (2, 5825)\\t1\\n ...</td>\n",
       "      <td>(0, 1235)\\t1.0\\n  (1, 2)\\t1.0\\n  (2, 5825)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Abuja</td>\n",
       "      <td>107</td>\n",
       "      <td>828.0</td>\n",
       "      <td>209.646135</td>\n",
       "      <td>27.042271</td>\n",
       "      <td>5.580918</td>\n",
       "      <td>1.437198</td>\n",
       "      <td>7.075491</td>\n",
       "      <td>15.369565</td>\n",
       "      <td>5.787440</td>\n",
       "      <td>0.404589</td>\n",
       "      <td>1.263285</td>\n",
       "      <td>0.018193</td>\n",
       "      <td>0.856400</td>\n",
       "      <td>0.125396</td>\n",
       "      <td>0.356573</td>\n",
       "      <td>0.186980</td>\n",
       "      <td>0.365899</td>\n",
       "      <td>'Brother #Abuja https://t.co/6J3yOIP8Jm', 'Our...</td>\n",
       "      <td>180295</td>\n",
       "      <td>4561</td>\n",
       "      <td>18553</td>\n",
       "      <td>3862</td>\n",
       "      <td>232</td>\n",
       "      <td>766</td>\n",
       "      <td>brother abuja httpsSkeptical_annoyed_undecided...</td>\n",
       "      <td>15083</td>\n",
       "      <td>182674</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.249437</td>\n",
       "      <td>0.498789</td>\n",
       "      <td>[brother, abuja, httpsSkeptical_annoyed_undeci...</td>\n",
       "      <td>[(brother, NN), (abuja, NN), (httpsSkeptical_a...</td>\n",
       "      <td>[(abuja, 635), (lagos, 163), (abujatwittercomm...</td>\n",
       "      <td>(0, 732)\\t1\\n  (1, 24)\\t1\\n  (2, 2539)\\t1\\n ...</td>\n",
       "      <td>(0, 732)\\t1.0\\n  (1, 24)\\t1.0\\n  (2, 2539)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Amsterdam</td>\n",
       "      <td>9</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>180.311000</td>\n",
       "      <td>22.627000</td>\n",
       "      <td>5.220000</td>\n",
       "      <td>1.520000</td>\n",
       "      <td>7.579219</td>\n",
       "      <td>14.554000</td>\n",
       "      <td>5.239000</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>0.663000</td>\n",
       "      <td>0.024567</td>\n",
       "      <td>0.875009</td>\n",
       "      <td>0.100439</td>\n",
       "      <td>0.250798</td>\n",
       "      <td>0.152442</td>\n",
       "      <td>0.316410</td>\n",
       "      <td>'Herfst in #Amsterdam 🍂🍃🍁 https://t.co/F0VKRmq...</td>\n",
       "      <td>185912</td>\n",
       "      <td>5200</td>\n",
       "      <td>18994</td>\n",
       "      <td>4740</td>\n",
       "      <td>134</td>\n",
       "      <td>563</td>\n",
       "      <td>herfst amsterdam fallen_leafleaf_fluttering_in...</td>\n",
       "      <td>15579</td>\n",
       "      <td>197584</td>\n",
       "      <td>11.9</td>\n",
       "      <td>0.218700</td>\n",
       "      <td>0.476012</td>\n",
       "      <td>[herfst, amsterdam, fallen_leafleaf_fluttering...</td>\n",
       "      <td>[(herfst, NN), (amsterdam, NN), (fallen_leafle...</td>\n",
       "      <td>[(amsterdam, 993), (netherlands, 80), (day, 64...</td>\n",
       "      <td>(0, 1939)\\t1\\n  (1, 171)\\t1\\n  (2, 1517)\\t1\\...</td>\n",
       "      <td>(0, 1939)\\t1.0\\n  (1, 171)\\t1.0\\n  (2, 1517)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Ankara</td>\n",
       "      <td>57</td>\n",
       "      <td>313.0</td>\n",
       "      <td>185.808307</td>\n",
       "      <td>22.984026</td>\n",
       "      <td>3.974441</td>\n",
       "      <td>1.306709</td>\n",
       "      <td>7.510589</td>\n",
       "      <td>14.747604</td>\n",
       "      <td>5.623003</td>\n",
       "      <td>0.338658</td>\n",
       "      <td>0.840256</td>\n",
       "      <td>0.039681</td>\n",
       "      <td>0.865914</td>\n",
       "      <td>0.094422</td>\n",
       "      <td>0.192013</td>\n",
       "      <td>0.128041</td>\n",
       "      <td>0.241076</td>\n",
       "      <td>'I had a great and very joyful flight today ✈️...</td>\n",
       "      <td>60230</td>\n",
       "      <td>1241</td>\n",
       "      <td>6328</td>\n",
       "      <td>1426</td>\n",
       "      <td>77</td>\n",
       "      <td>224</td>\n",
       "      <td>great joyful flight today airplane nto ankara ...</td>\n",
       "      <td>5302</td>\n",
       "      <td>66000</td>\n",
       "      <td>11.7</td>\n",
       "      <td>0.164460</td>\n",
       "      <td>0.419755</td>\n",
       "      <td>[great, joyful, flight, today, airplane, nto, ...</td>\n",
       "      <td>[(great, JJ), (joyful, JJ), (flight, NN), (tod...</td>\n",
       "      <td>[(ankara, 251), (turkey, 77), (turkish, 26), (...</td>\n",
       "      <td>(0, 1059)\\t1\\n  (1, 1592)\\t1\\n  (2, 952)\\t1\\...</td>\n",
       "      <td>(0, 1059)\\t1.0\\n  (1, 1592)\\t1.0\\n  (2, 952)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Athens</td>\n",
       "      <td>99</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>186.752000</td>\n",
       "      <td>24.268000</td>\n",
       "      <td>5.405000</td>\n",
       "      <td>1.503000</td>\n",
       "      <td>7.170592</td>\n",
       "      <td>15.201000</td>\n",
       "      <td>5.442000</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.042164</td>\n",
       "      <td>0.865646</td>\n",
       "      <td>0.092190</td>\n",
       "      <td>0.176392</td>\n",
       "      <td>0.126462</td>\n",
       "      <td>0.320793</td>\n",
       "      <td>'Georgia GA #GARunoffs\\n#Sparta #Athens #Ciara...</td>\n",
       "      <td>192839</td>\n",
       "      <td>5370</td>\n",
       "      <td>20229</td>\n",
       "      <td>4795</td>\n",
       "      <td>180</td>\n",
       "      <td>766</td>\n",
       "      <td>georgia ga garunoffsnsparta athens ciara lukes...</td>\n",
       "      <td>16782</td>\n",
       "      <td>204722</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.173722</td>\n",
       "      <td>0.448072</td>\n",
       "      <td>[georgia, ga, garunoffsnsparta, athens, ciara,...</td>\n",
       "      <td>[(georgia, NN), (ga, NN), (garunoffsnsparta, N...</td>\n",
       "      <td>[(athens, 867), (greece, 322), (greek, 74), (a...</td>\n",
       "      <td>(0, 1653)\\t1\\n  (1, 1613)\\t1\\n  (2, 1628)\\t1...</td>\n",
       "      <td>(0, 1653)\\t1.0\\n  (1, 1613)\\t1.0\\n  (2, 1628...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      city_id rank2020  num_tweets  avg_tweet_len  avg_num_words  \\\n",
       "0   #AbuDhabi       42       832.0     193.930288      25.002404   \n",
       "1      #Abuja      107       828.0     209.646135      27.042271   \n",
       "2  #Amsterdam        9      1000.0     180.311000      22.627000   \n",
       "3     #Ankara       57       313.0     185.808307      22.984026   \n",
       "4     #Athens       99      1000.0     186.752000      24.268000   \n",
       "\n",
       "   avg_stopwords  avg_num_sentences  avg_word_len  avg_punctuation  \\\n",
       "0       5.794471           1.435096      7.076026        14.016827   \n",
       "1       5.580918           1.437198      7.075491        15.369565   \n",
       "2       5.220000           1.520000      7.579219        14.554000   \n",
       "3       3.974441           1.306709      7.510589        14.747604   \n",
       "4       5.405000           1.503000      7.170592        15.201000   \n",
       "\n",
       "   avg_hashtags  avg_numerics  avg_upper   avg_neg   avg_neu   avg_pos  \\\n",
       "0      4.602163      0.278846   1.091346  0.021933  0.860132  0.117936   \n",
       "1      5.787440      0.404589   1.263285  0.018193  0.856400  0.125396   \n",
       "2      5.239000      0.153000   0.663000  0.024567  0.875009  0.100439   \n",
       "3      5.623003      0.338658   0.840256  0.039681  0.865914  0.094422   \n",
       "4      5.442000      0.226000   0.865000  0.042164  0.865646  0.092190   \n",
       "\n",
       "   avg_compound  avg_polarity  avg_subjectivity  \\\n",
       "0      0.299883      0.189999          0.349085   \n",
       "1      0.356573      0.186980          0.365899   \n",
       "2      0.250798      0.152442          0.316410   \n",
       "3      0.192013      0.128041          0.241076   \n",
       "4      0.176392      0.126462          0.320793   \n",
       "\n",
       "                                       tweets_merged  words_total  \\\n",
       "0  'A Delegation from  @aau_ae visited @BurjeelMe...       166602   \n",
       "1  'Brother #Abuja https://t.co/6J3yOIP8Jm', 'Our...       180295   \n",
       "2  'Herfst in #Amsterdam 🍂🍃🍁 https://t.co/F0VKRmq...       185912   \n",
       "3  'I had a great and very joyful flight today ✈️...        60230   \n",
       "4  'Georgia GA #GARunoffs\\n#Sparta #Athens #Ciara...       192839   \n",
       "\n",
       "   stopwords_total  punctuation_total  hashtags_total  numerics_total  \\\n",
       "0             4811              15703            3268             202   \n",
       "1             4561              18553            3862             232   \n",
       "2             5200              18994            4740             134   \n",
       "3             1241               6328            1426              77   \n",
       "4             5370              20229            4795             180   \n",
       "\n",
       "   upper_total                                         clean_text  \\\n",
       "0          760  delegation aau_ae visited burjeelmedicity one ...   \n",
       "1          766  brother abuja httpsSkeptical_annoyed_undecided...   \n",
       "2          563  herfst amsterdam fallen_leafleaf_fluttering_in...   \n",
       "3          224  great joyful flight today airplane nto ankara ...   \n",
       "4          766  georgia ga garunoffsnsparta athens ciara lukes...   \n",
       "\n",
       "   clean_words_total  clean_char_total  avg_clean_word_len  polarity_clean  \\\n",
       "0              14484            173030                11.2        0.242483   \n",
       "1              15083            182674                11.4        0.249437   \n",
       "2              15579            197584                11.9        0.218700   \n",
       "3               5302             66000                11.7        0.164460   \n",
       "4              16782            204722                11.5        0.173722   \n",
       "\n",
       "   subjectivity_clean                                             tokens  \\\n",
       "0            0.471562  [delegation, aau_ae, visited, burjeelmedicity,...   \n",
       "1            0.498789  [brother, abuja, httpsSkeptical_annoyed_undeci...   \n",
       "2            0.476012  [herfst, amsterdam, fallen_leafleaf_fluttering...   \n",
       "3            0.419755  [great, joyful, flight, today, airplane, nto, ...   \n",
       "4            0.448072  [georgia, ga, garunoffsnsparta, athens, ciara,...   \n",
       "\n",
       "                                             pos_tag  \\\n",
       "0  [(delegation, NN), (aau_ae, NN), (visited, VBD...   \n",
       "1  [(brother, NN), (abuja, NN), (httpsSkeptical_a...   \n",
       "2  [(herfst, NN), (amsterdam, NN), (fallen_leafle...   \n",
       "3  [(great, JJ), (joyful, JJ), (flight, NN), (tod...   \n",
       "4  [(georgia, NN), (ga, NN), (garunoffsnsparta, N...   \n",
       "\n",
       "                                        most_freq100  \\\n",
       "0  [(abudhabi, 677), (uae, 224), (abu, 156), (dub...   \n",
       "1  [(abuja, 635), (lagos, 163), (abujatwittercomm...   \n",
       "2  [(amsterdam, 993), (netherlands, 80), (day, 64...   \n",
       "3  [(ankara, 251), (turkey, 77), (turkish, 26), (...   \n",
       "4  [(athens, 867), (greece, 322), (greek, 74), (a...   \n",
       "\n",
       "                                           count_vec  \\\n",
       "0    (0, 1235)\\t1\\n  (1, 2)\\t1\\n  (2, 5825)\\t1\\n ...   \n",
       "1    (0, 732)\\t1\\n  (1, 24)\\t1\\n  (2, 2539)\\t1\\n ...   \n",
       "2    (0, 1939)\\t1\\n  (1, 171)\\t1\\n  (2, 1517)\\t1\\...   \n",
       "3    (0, 1059)\\t1\\n  (1, 1592)\\t1\\n  (2, 952)\\t1\\...   \n",
       "4    (0, 1653)\\t1\\n  (1, 1613)\\t1\\n  (2, 1628)\\t1...   \n",
       "\n",
       "                                               tfidf  \n",
       "0    (0, 1235)\\t1.0\\n  (1, 2)\\t1.0\\n  (2, 5825)\\t...  \n",
       "1    (0, 732)\\t1.0\\n  (1, 24)\\t1.0\\n  (2, 2539)\\t...  \n",
       "2    (0, 1939)\\t1.0\\n  (1, 171)\\t1.0\\n  (2, 1517)...  \n",
       "3    (0, 1059)\\t1.0\\n  (1, 1592)\\t1.0\\n  (2, 952)...  \n",
       "4    (0, 1653)\\t1.0\\n  (1, 1613)\\t1.0\\n  (2, 1628...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean column names\n",
    "sc100_features.columns = ['city_id', 'rank2020', 'num_tweets','avg_tweet_len', 'avg_num_words','avg_stopwords', 'avg_num_sentences', 'avg_word_len', 'avg_punctuation', 'avg_hashtags', 'avg_numerics', 'avg_upper', 'avg_neg', 'avg_neu', 'avg_pos', 'avg_compound', 'avg_polarity', 'avg_subjectivity', 'tweets_merged', 'words_total', 'stopwords_total', 'punctuation_total','hashtags_total','numerics_total','upper_total','clean_text','clean_words_total','clean_char_total','avg_clean_word_len','polarity_clean','subjectivity_clean','tokens','pos_tag','most_freq100','count_vec','tfidf']\n",
    "sc100_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean city_id by removing # using regex\n",
    "sc100_features['city_id'] = sc100_features['city_id'].apply(lambda x: re.sub(r'#','', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower characters\n",
    "sc100_features['city_id'] = sc100_features['city_id'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_id</th>\n",
       "      <th>rank2020</th>\n",
       "      <th>num_tweets</th>\n",
       "      <th>avg_tweet_len</th>\n",
       "      <th>avg_num_words</th>\n",
       "      <th>avg_stopwords</th>\n",
       "      <th>avg_num_sentences</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>avg_punctuation</th>\n",
       "      <th>avg_hashtags</th>\n",
       "      <th>avg_numerics</th>\n",
       "      <th>avg_upper</th>\n",
       "      <th>avg_neg</th>\n",
       "      <th>avg_neu</th>\n",
       "      <th>avg_pos</th>\n",
       "      <th>avg_compound</th>\n",
       "      <th>avg_polarity</th>\n",
       "      <th>avg_subjectivity</th>\n",
       "      <th>tweets_merged</th>\n",
       "      <th>words_total</th>\n",
       "      <th>stopwords_total</th>\n",
       "      <th>punctuation_total</th>\n",
       "      <th>hashtags_total</th>\n",
       "      <th>numerics_total</th>\n",
       "      <th>upper_total</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_words_total</th>\n",
       "      <th>clean_char_total</th>\n",
       "      <th>avg_clean_word_len</th>\n",
       "      <th>polarity_clean</th>\n",
       "      <th>subjectivity_clean</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>most_freq100</th>\n",
       "      <th>count_vec</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abudhabi</td>\n",
       "      <td>42</td>\n",
       "      <td>832</td>\n",
       "      <td>193.930288</td>\n",
       "      <td>25.002404</td>\n",
       "      <td>5.794471</td>\n",
       "      <td>1.435096</td>\n",
       "      <td>7.076026</td>\n",
       "      <td>14.016827</td>\n",
       "      <td>4.602163</td>\n",
       "      <td>0.278846</td>\n",
       "      <td>1.091346</td>\n",
       "      <td>0.021933</td>\n",
       "      <td>0.860132</td>\n",
       "      <td>0.117936</td>\n",
       "      <td>0.299883</td>\n",
       "      <td>0.189999</td>\n",
       "      <td>0.349085</td>\n",
       "      <td>'A Delegation from  @aau_ae visited @BurjeelMe...</td>\n",
       "      <td>166602</td>\n",
       "      <td>4811</td>\n",
       "      <td>15703</td>\n",
       "      <td>3268</td>\n",
       "      <td>202</td>\n",
       "      <td>760</td>\n",
       "      <td>delegation aau_ae visited burjeelmedicity one ...</td>\n",
       "      <td>14484</td>\n",
       "      <td>173030</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.242483</td>\n",
       "      <td>0.471562</td>\n",
       "      <td>[delegation, aau_ae, visited, burjeelmedicity,...</td>\n",
       "      <td>[(delegation, NN), (aau_ae, NN), (visited, VBD...</td>\n",
       "      <td>[(abudhabi, 677), (uae, 224), (abu, 156), (dub...</td>\n",
       "      <td>(0, 1235)\\t1\\n  (1, 2)\\t1\\n  (2, 5825)\\t1\\n ...</td>\n",
       "      <td>(0, 1235)\\t1.0\\n  (1, 2)\\t1.0\\n  (2, 5825)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abuja</td>\n",
       "      <td>107</td>\n",
       "      <td>828</td>\n",
       "      <td>209.646135</td>\n",
       "      <td>27.042271</td>\n",
       "      <td>5.580918</td>\n",
       "      <td>1.437198</td>\n",
       "      <td>7.075491</td>\n",
       "      <td>15.369565</td>\n",
       "      <td>5.787440</td>\n",
       "      <td>0.404589</td>\n",
       "      <td>1.263285</td>\n",
       "      <td>0.018193</td>\n",
       "      <td>0.856400</td>\n",
       "      <td>0.125396</td>\n",
       "      <td>0.356573</td>\n",
       "      <td>0.186980</td>\n",
       "      <td>0.365899</td>\n",
       "      <td>'Brother #Abuja https://t.co/6J3yOIP8Jm', 'Our...</td>\n",
       "      <td>180295</td>\n",
       "      <td>4561</td>\n",
       "      <td>18553</td>\n",
       "      <td>3862</td>\n",
       "      <td>232</td>\n",
       "      <td>766</td>\n",
       "      <td>brother abuja httpsSkeptical_annoyed_undecided...</td>\n",
       "      <td>15083</td>\n",
       "      <td>182674</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.249437</td>\n",
       "      <td>0.498789</td>\n",
       "      <td>[brother, abuja, httpsSkeptical_annoyed_undeci...</td>\n",
       "      <td>[(brother, NN), (abuja, NN), (httpsSkeptical_a...</td>\n",
       "      <td>[(abuja, 635), (lagos, 163), (abujatwittercomm...</td>\n",
       "      <td>(0, 732)\\t1\\n  (1, 24)\\t1\\n  (2, 2539)\\t1\\n ...</td>\n",
       "      <td>(0, 732)\\t1.0\\n  (1, 24)\\t1.0\\n  (2, 2539)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amsterdam</td>\n",
       "      <td>9</td>\n",
       "      <td>1000</td>\n",
       "      <td>180.311000</td>\n",
       "      <td>22.627000</td>\n",
       "      <td>5.220000</td>\n",
       "      <td>1.520000</td>\n",
       "      <td>7.579219</td>\n",
       "      <td>14.554000</td>\n",
       "      <td>5.239000</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>0.663000</td>\n",
       "      <td>0.024567</td>\n",
       "      <td>0.875009</td>\n",
       "      <td>0.100439</td>\n",
       "      <td>0.250798</td>\n",
       "      <td>0.152442</td>\n",
       "      <td>0.316410</td>\n",
       "      <td>'Herfst in #Amsterdam 🍂🍃🍁 https://t.co/F0VKRmq...</td>\n",
       "      <td>185912</td>\n",
       "      <td>5200</td>\n",
       "      <td>18994</td>\n",
       "      <td>4740</td>\n",
       "      <td>134</td>\n",
       "      <td>563</td>\n",
       "      <td>herfst amsterdam fallen_leafleaf_fluttering_in...</td>\n",
       "      <td>15579</td>\n",
       "      <td>197584</td>\n",
       "      <td>11.9</td>\n",
       "      <td>0.218700</td>\n",
       "      <td>0.476012</td>\n",
       "      <td>[herfst, amsterdam, fallen_leafleaf_fluttering...</td>\n",
       "      <td>[(herfst, NN), (amsterdam, NN), (fallen_leafle...</td>\n",
       "      <td>[(amsterdam, 993), (netherlands, 80), (day, 64...</td>\n",
       "      <td>(0, 1939)\\t1\\n  (1, 171)\\t1\\n  (2, 1517)\\t1\\...</td>\n",
       "      <td>(0, 1939)\\t1.0\\n  (1, 171)\\t1.0\\n  (2, 1517)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ankara</td>\n",
       "      <td>57</td>\n",
       "      <td>313</td>\n",
       "      <td>185.808307</td>\n",
       "      <td>22.984026</td>\n",
       "      <td>3.974441</td>\n",
       "      <td>1.306709</td>\n",
       "      <td>7.510589</td>\n",
       "      <td>14.747604</td>\n",
       "      <td>5.623003</td>\n",
       "      <td>0.338658</td>\n",
       "      <td>0.840256</td>\n",
       "      <td>0.039681</td>\n",
       "      <td>0.865914</td>\n",
       "      <td>0.094422</td>\n",
       "      <td>0.192013</td>\n",
       "      <td>0.128041</td>\n",
       "      <td>0.241076</td>\n",
       "      <td>'I had a great and very joyful flight today ✈️...</td>\n",
       "      <td>60230</td>\n",
       "      <td>1241</td>\n",
       "      <td>6328</td>\n",
       "      <td>1426</td>\n",
       "      <td>77</td>\n",
       "      <td>224</td>\n",
       "      <td>great joyful flight today airplane nto ankara ...</td>\n",
       "      <td>5302</td>\n",
       "      <td>66000</td>\n",
       "      <td>11.7</td>\n",
       "      <td>0.164460</td>\n",
       "      <td>0.419755</td>\n",
       "      <td>[great, joyful, flight, today, airplane, nto, ...</td>\n",
       "      <td>[(great, JJ), (joyful, JJ), (flight, NN), (tod...</td>\n",
       "      <td>[(ankara, 251), (turkey, 77), (turkish, 26), (...</td>\n",
       "      <td>(0, 1059)\\t1\\n  (1, 1592)\\t1\\n  (2, 952)\\t1\\...</td>\n",
       "      <td>(0, 1059)\\t1.0\\n  (1, 1592)\\t1.0\\n  (2, 952)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>athens</td>\n",
       "      <td>99</td>\n",
       "      <td>1000</td>\n",
       "      <td>186.752000</td>\n",
       "      <td>24.268000</td>\n",
       "      <td>5.405000</td>\n",
       "      <td>1.503000</td>\n",
       "      <td>7.170592</td>\n",
       "      <td>15.201000</td>\n",
       "      <td>5.442000</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.042164</td>\n",
       "      <td>0.865646</td>\n",
       "      <td>0.092190</td>\n",
       "      <td>0.176392</td>\n",
       "      <td>0.126462</td>\n",
       "      <td>0.320793</td>\n",
       "      <td>'Georgia GA #GARunoffs\\n#Sparta #Athens #Ciara...</td>\n",
       "      <td>192839</td>\n",
       "      <td>5370</td>\n",
       "      <td>20229</td>\n",
       "      <td>4795</td>\n",
       "      <td>180</td>\n",
       "      <td>766</td>\n",
       "      <td>georgia ga garunoffsnsparta athens ciara lukes...</td>\n",
       "      <td>16782</td>\n",
       "      <td>204722</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.173722</td>\n",
       "      <td>0.448072</td>\n",
       "      <td>[georgia, ga, garunoffsnsparta, athens, ciara,...</td>\n",
       "      <td>[(georgia, NN), (ga, NN), (garunoffsnsparta, N...</td>\n",
       "      <td>[(athens, 867), (greece, 322), (greek, 74), (a...</td>\n",
       "      <td>(0, 1653)\\t1\\n  (1, 1613)\\t1\\n  (2, 1628)\\t1...</td>\n",
       "      <td>(0, 1653)\\t1.0\\n  (1, 1613)\\t1.0\\n  (2, 1628...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     city_id rank2020  num_tweets  avg_tweet_len  avg_num_words  \\\n",
       "0   abudhabi       42         832     193.930288      25.002404   \n",
       "1      abuja      107         828     209.646135      27.042271   \n",
       "2  amsterdam        9        1000     180.311000      22.627000   \n",
       "3     ankara       57         313     185.808307      22.984026   \n",
       "4     athens       99        1000     186.752000      24.268000   \n",
       "\n",
       "   avg_stopwords  avg_num_sentences  avg_word_len  avg_punctuation  \\\n",
       "0       5.794471           1.435096      7.076026        14.016827   \n",
       "1       5.580918           1.437198      7.075491        15.369565   \n",
       "2       5.220000           1.520000      7.579219        14.554000   \n",
       "3       3.974441           1.306709      7.510589        14.747604   \n",
       "4       5.405000           1.503000      7.170592        15.201000   \n",
       "\n",
       "   avg_hashtags  avg_numerics  avg_upper   avg_neg   avg_neu   avg_pos  \\\n",
       "0      4.602163      0.278846   1.091346  0.021933  0.860132  0.117936   \n",
       "1      5.787440      0.404589   1.263285  0.018193  0.856400  0.125396   \n",
       "2      5.239000      0.153000   0.663000  0.024567  0.875009  0.100439   \n",
       "3      5.623003      0.338658   0.840256  0.039681  0.865914  0.094422   \n",
       "4      5.442000      0.226000   0.865000  0.042164  0.865646  0.092190   \n",
       "\n",
       "   avg_compound  avg_polarity  avg_subjectivity  \\\n",
       "0      0.299883      0.189999          0.349085   \n",
       "1      0.356573      0.186980          0.365899   \n",
       "2      0.250798      0.152442          0.316410   \n",
       "3      0.192013      0.128041          0.241076   \n",
       "4      0.176392      0.126462          0.320793   \n",
       "\n",
       "                                       tweets_merged  words_total  \\\n",
       "0  'A Delegation from  @aau_ae visited @BurjeelMe...       166602   \n",
       "1  'Brother #Abuja https://t.co/6J3yOIP8Jm', 'Our...       180295   \n",
       "2  'Herfst in #Amsterdam 🍂🍃🍁 https://t.co/F0VKRmq...       185912   \n",
       "3  'I had a great and very joyful flight today ✈️...        60230   \n",
       "4  'Georgia GA #GARunoffs\\n#Sparta #Athens #Ciara...       192839   \n",
       "\n",
       "   stopwords_total  punctuation_total  hashtags_total  numerics_total  \\\n",
       "0             4811              15703            3268             202   \n",
       "1             4561              18553            3862             232   \n",
       "2             5200              18994            4740             134   \n",
       "3             1241               6328            1426              77   \n",
       "4             5370              20229            4795             180   \n",
       "\n",
       "   upper_total                                         clean_text  \\\n",
       "0          760  delegation aau_ae visited burjeelmedicity one ...   \n",
       "1          766  brother abuja httpsSkeptical_annoyed_undecided...   \n",
       "2          563  herfst amsterdam fallen_leafleaf_fluttering_in...   \n",
       "3          224  great joyful flight today airplane nto ankara ...   \n",
       "4          766  georgia ga garunoffsnsparta athens ciara lukes...   \n",
       "\n",
       "   clean_words_total  clean_char_total  avg_clean_word_len  polarity_clean  \\\n",
       "0              14484            173030                11.2        0.242483   \n",
       "1              15083            182674                11.4        0.249437   \n",
       "2              15579            197584                11.9        0.218700   \n",
       "3               5302             66000                11.7        0.164460   \n",
       "4              16782            204722                11.5        0.173722   \n",
       "\n",
       "   subjectivity_clean                                             tokens  \\\n",
       "0            0.471562  [delegation, aau_ae, visited, burjeelmedicity,...   \n",
       "1            0.498789  [brother, abuja, httpsSkeptical_annoyed_undeci...   \n",
       "2            0.476012  [herfst, amsterdam, fallen_leafleaf_fluttering...   \n",
       "3            0.419755  [great, joyful, flight, today, airplane, nto, ...   \n",
       "4            0.448072  [georgia, ga, garunoffsnsparta, athens, ciara,...   \n",
       "\n",
       "                                             pos_tag  \\\n",
       "0  [(delegation, NN), (aau_ae, NN), (visited, VBD...   \n",
       "1  [(brother, NN), (abuja, NN), (httpsSkeptical_a...   \n",
       "2  [(herfst, NN), (amsterdam, NN), (fallen_leafle...   \n",
       "3  [(great, JJ), (joyful, JJ), (flight, NN), (tod...   \n",
       "4  [(georgia, NN), (ga, NN), (garunoffsnsparta, N...   \n",
       "\n",
       "                                        most_freq100  \\\n",
       "0  [(abudhabi, 677), (uae, 224), (abu, 156), (dub...   \n",
       "1  [(abuja, 635), (lagos, 163), (abujatwittercomm...   \n",
       "2  [(amsterdam, 993), (netherlands, 80), (day, 64...   \n",
       "3  [(ankara, 251), (turkey, 77), (turkish, 26), (...   \n",
       "4  [(athens, 867), (greece, 322), (greek, 74), (a...   \n",
       "\n",
       "                                           count_vec  \\\n",
       "0    (0, 1235)\\t1\\n  (1, 2)\\t1\\n  (2, 5825)\\t1\\n ...   \n",
       "1    (0, 732)\\t1\\n  (1, 24)\\t1\\n  (2, 2539)\\t1\\n ...   \n",
       "2    (0, 1939)\\t1\\n  (1, 171)\\t1\\n  (2, 1517)\\t1\\...   \n",
       "3    (0, 1059)\\t1\\n  (1, 1592)\\t1\\n  (2, 952)\\t1\\...   \n",
       "4    (0, 1653)\\t1\\n  (1, 1613)\\t1\\n  (2, 1628)\\t1...   \n",
       "\n",
       "                                               tfidf  \n",
       "0    (0, 1235)\\t1.0\\n  (1, 2)\\t1.0\\n  (2, 5825)\\t...  \n",
       "1    (0, 732)\\t1.0\\n  (1, 24)\\t1.0\\n  (2, 2539)\\t...  \n",
       "2    (0, 1939)\\t1.0\\n  (1, 171)\\t1.0\\n  (2, 1517)...  \n",
       "3    (0, 1059)\\t1.0\\n  (1, 1592)\\t1.0\\n  (2, 952)...  \n",
       "4    (0, 1653)\\t1.0\\n  (1, 1613)\\t1.0\\n  (2, 1628...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc100_features['num_tweets'] = sc100_features['num_tweets'].apply(np.int64)\n",
    "sc100_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract weigh of BoW for each city_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data transformations\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vect = CountVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "\n",
    "# define each BoW as a list of thematic words (Lexicons)\n",
    "smartcity = ['digitaltransformation','2.0','smartcity','iot','smart','city','ai','smartcities','innovation','cities','5g','data','bigdata','technology','future','tech','digital','digitaltransformation','machinelearning','analytics','solutions','cloud','datascience','network','security','cybersecurity','urban','people','artificialintelligence','smarthome','internetofthings','autonomous','smart','technologies','robotics','software','research','monitoring','street','sensors','futureofwork','planning','governance','transportation','safecity','building','robot','industry40','startups','development','smartthings','urbanplanning','connected','internet','sustainable','waste','emergingtech','programming','environment','safety','lighting','safety','roads','workplaces','smartbuildings','driverless','selfdriving','smartmobility','futureworkspace','digitalcity','deeplearning','datadriven','application','robots','publicsafety','energyefficiency','labgov','citizens','transformation','controlroom','publicspace','airquality','efficiency','era','smarter','urbaninnovation','internetofeverything','newtown','iotforall','construction','privacy','surveillance','drones','smartbuilding','smartgrid','smartgrids','cybersecurity','policy','powergrid','smartcities','iot','electricity','smartmetering','smartmeters','technology','microgrids','microgrid','smartcity','powergrids','iot','ai','machinelearning','bigdata','serverless','cybersecurity','devcommunity','programming','coding','cloudcomputing','codenewbie','100daysofmlcode','womenintech','innovation','deeplearning','security','machine','digital','industrial','artificial','computing','robot','wearables','artificial','networks','drones','connectivity','wireless','urbanplanning','city','urban','cities','smartcities','architecture','urbanism','urbandesign','cityplanning','urbanization','futurecities','urbaninnovation','bigdata','cityplanner','architects','citylab','urbanplanner','urbanplanners','cityplanners','urbandata','townplanning','planners','neighborhoods','sustainablecities','tacticalurbanism','urbanmobility','pedestrians','neighborhood','citiesforpeople','neighbourhood','rethinkreplace','constructionmanagement','suburban','futurecity','cityvisionnaire','smarttechnology','citygoals','civilengineering','digitaltwin','bigdata','digitaltransformation','pedestrian-friendly','surveillance','technologies','urbandevelopment','cities','urban','urbanplanning'] \n",
    "civictech = ['aiethics','empowerment','learned','communityengagement','communitydevelopment','nonprofit','notforprofit','collaboration','vulnerable','diversity','egov','egovernance','inclusion','ethics','social','engagement','crowdsourcing','humanrights','citizenship','civic','opendata','referendum','rights','citizens','open','social','people','sharingeconomy','hack','citizens','communities','civictech','inequalities','equity','participatory','social','youthempowerment','govtech','localgov','civic','stateandlocal','community','governmentit','statelocalit','opendata','sharing','including','volunteer','social','people','shared','learn','opengov','localgovernment','education','hackathon','civictechto','techforgood','discussion','communities','local','digitalgov','future','online','governments','webmapping','capacity','accessibility','discuss','ethics','citizens','together','initiative','transparency','data4good','publicinteresttech','democracy','volunteers','society','makers','ecitizenship','eresidency','citizenship','egovernment','esafety','digitaltransformation','policy','social','technology','future','citizen','ecivility','digitaleconomy','ideagov','egov','people','edemocracy','eparticipation','education','citizens','progressive','security','ecitizenship','inclusive','watch','startups','concept','digitalnation','netgov','app','privacy','egovernance','tool','democratic','eublockchain','digitaleu','digitalnomad','apps','residents','populationeu','migrationpolicy','migrationsystem','migration','voted','govtech','smartcities','humandevelopment','erisks','ecitizen','artificial','intelligence','opendata','empowerment','leadership','growth','success','youth','wisdom','education','relationship','powerinpurpose','community','beyondlimits','powerwithin','support','empower','empowering','equality','enlightment','selfdiscovery','confidence','selfempowerment','leaders','forward','diversity','empowered','successmindset','cybersecurity','government','security','technology','opendata','open','openscience','opensource','opendataday','community','citizenscience','citizen','citizenship','openaccess','transparence','openbanking','womenwhocode','openapis','policy','grants','decisions','institutions','opengov','social','busopendata','datadriven','citizen','openstreetmap','researchdata','engagement','citizens']\n",
    "infrastructure = ['supply','platform','blockchain','system','internet','battery','batterystorage','energystorage','mobility','storage','blockchain','serverless','grids','blockchain','standards','realestate','construction','driverless','selfdriving','smartmobility','transportation','roads', 'building','traffic','autonomousvehicles','streets','platform','transportation','road','publicspace','realestate','monuments','housing','selfdrivingcars','high-speed','real-estate','mobility','highways','transportation','transport','automotive','transit','buses','cars','futureofmobility','car','travel','publictransport','autonomous','bus','auto','shared','traffic','delivery','emissions','5g','services','connectivity','van','trucks','vehicles','commute','electricvehicles','smartmobility','vehicle','logistics','multimodal','connected','metropolitan','efficiency','monitoring','providers','mobile','futurride','autonomousvehicle','engineer','seat','robotics','parking','cycling','driver','flying','rail','moving','wheelchair','automobile','walks','electricvehicle','electriccars','platform','cctv','drivers','driverless','systems','roads','drones','aviation','selfdriving','robots','electriccar','device','bike','autos','urbanmobility','speed','selfdrivingcars','network','street','urbanplanning','autocar','drive','electrification','streets','driverlesscars','bicycle','zero-emission','engineering','artificialintelligence','emobility','fleet','scooters','bikes','journey','manufacturers','route','logistics','driver','trucking','delivery','shipping','traffic','transport','freight','rentals','road','drivers','bus','mobility','rail','cargo','cars','trucks','trucker','fleet','mechanic','infrastructure','railways','diesel','taxi','truckdriver','truckers','shipments','driving','flights','pickup','airfreight','intelligentlogistics','bicycle','cycling','port','wheels','motors','machinery','ferry','infrastructure','bridge','network','architecture','building','construction','transport','metro','roads','highways','railways','realestate','housing','train','civilengineering','airport','station','connectivity','stations','tunnel','airports','equipment','bridges','km','electrical','highway','tunnels','road']\n",
    "sustainability = ['resource','resources','sustainablefinance','cyberresilience','recycling','resilient','batteries','emission','resilience','energytransition','sustainability','biodiversity','sustainabledevelopment','resources','sustainableurbanplanning','sustainabletransport','sustainable','environment','climatechange','energy','climate','innovation','circulareconomy','environmental','ecofriendly','nature','planet','carbon','recycling','climateaction','plastic','renewableenergy','zerowaste','water','waste','solar','agriculture','recycle','savetheplanet','pollution','emissions','sustainablefashion','hydrogen','eco','sustainableliving','reuse','clean','recycled','greenliving','renewable','circular','gogreen','biodiversity','sustainable','renewables','climatecrisis','organic','eco-friendly','earth','sustainabledevelopment','electric','electricity','efficiency','electricvehicles','recyclable','footprint','sustainablefinance','ocean','greener','co2','plasticfree','climateemergency','greenenergy','waterconservation','carbonfootprint','greenbuilding','energytransition','sustainablity','consumption','globalwarming','resilience','reusable','battery','decompose','aquaculture','oil','compost','circularity','plastics','environment','ecoconscious','savetheplanet','green_action','eco-outreach','eco-friendly','environmental_activism','climateactivism','energyefficiency','agtech','reduction','lessismore','responsibility','🌍','cleanup','cleantech','ethical','sustainably','ecofashion','foodtech','emobility','ecosystem','passivehouse','farmtotable','nature','wildlife','planet','ecofriendly','trees','conservation','climatecrisis','savetheplanet','clean','recycle','natural','biodiversity','tree','plants','emissions','reduce','ecology','zerowaste','forests','globalwarming','garden','species','gogreen','farming','agriculture','eco','plasticpollution','soil','greener','plasticfree','farmers','extinction','reuse','recycled','renewableenergy','circulareconomy','environmentally','deforestation','oceans','fuel','naturelovers','microplastics','airpollution','rivers','plastics','renewables','climateemergency','gas','sustainablefashion','environmentaljustice','eco-friendly','bees','fossil','wastewater','fornature','wildfires','greenhouse','ecosystems','renewables','oilandgas','hydrogen','cleanenergy','greenspaces','resilience','climateemergency','sustainablemobility','sustainabledevelopment','naturebasedsolutions','energyefficiency','actonclimate','solar','renewableenergy','alternativeenergy','solarpower','sustainable','environment','solarpanels','solarpv','scienceandenvironment','solarsystems','solarenergy','energymanagement','greennewdeal','solarenergy']\n",
    "governance = ['partnership','strategies','leadership','responsibility','regulations','ceo','policy','democracy2.0','elections','voter','collaboration','gov','governance','planning','management','localgov','project','partnership','partners','partner','leader','leaders','politicalparties','administration','decentralized','democracy2.0','political','society','minister','vision','crossbordercooperation','political','politics','democratic','government','politicians','elections','voter','public','commonwealth','politicalprogress','voting','debate','national','votingawareness','president','mayor','governance','parliament','campaign','liquiddemocracy','voters','brigade','constitutional','law','country','legalhack','decentralization','representatives','communityactivity','republican','policing','policy','policies','administration','governance','leadership','management','security','cybersecurity','ceo','corpgov','government','public','strategy','corruption','democracy','boardofdirectors','cio','policy','technology','leaders','boards','role','members','report','planning','throwback','organizations','national','directorship','structures','trust','directors','decentralized','countries','chair','discussion','conference','transparency','compensationcommittee','auditcommittee','cto','visionarytalks','riskmanagement','organisations','office','legal','regulations','audit','insights','corporategovernance','organization','voting','governing','organisation','policies','project','executive','responsible','leader','survey','interview','approach','staff','benefits','recruitment','regulatory','reporting','severity','obstacle','presidential','country','elections','institutions','alternative','digitaltransformation','freedom','nation','datavault','datawarehouse','regulation','processes','foundation','critical','control','candidate','promised','inauguration','anticorruption','executives','publicpolicy','principles','strategic','sharing','governments','federated','elected','framework','schedule','risks','certificate','meritocracy','stakeholders','partnership','society','equality','decisions','structure','implement','commitment','decision','driven','citiesalliance','assessment','corporations','cryptocurrencies','dataprotection']\n",
    "entrepreneurship = ['freemarket','investors','crypto','financial','accountability','enterprise','assetmanagement','accountable','company','finance','corporate','contractors','workforce','digitalbanking','entrepreneur','newmarket','investment', 'market','coding','startup','development','maker','fintech','economy','business','economic','finance','markets','investing','stocks','investment','gdp','money','financial','economics','trade','crypto','businesses','investors','retail','bitcoin','employment','job','startups','growth','work','entrepreneur','unemployment','entrepreneurship','sales','manufacturing','working','corporate','companies','funds','jobless','cryptocurrency','economicrecovery','smallbusiness','workers','investments','traders','budget','assets','growing','industries','asset','invest','startup','funding','development','currency','banks','monetary','debt','skills','fintech','fiscal','entrepreneurs','deficit','banking','benefits','rates','paying','billing','billingsoftware','earnings','workfromhome','accounting','personalfinance','ratings','accountant','treasury','equities','subsidies','opportunities','profits','stockstowatch','branding','marketing','customer','firms','localbusiness','capitalism','consumers','startup','startups','innovation','mindset','smallbusiness','businessowner','entrepreneurlife','digitalmarketing','management','founder','branding','growth','company','founders','funding','product','businesses','businessgrowth','sales','coaching','investment','entrepreneurial','venture','businessangel','capital','smallbusinessowner','businessman','customers','businesscoach','speaker','finance','market','skills','coding','ecommerce','entrepreneurmindset','startuplife','pitch','onlinebusiness','employment','businesswoman','socialmediamarketing','girlswhocode','crypto','womeninbusiness','investors','selfemployment','accelerator','incubator','designthinking','growthhacking','enterprise','changemakers','startupbusiness','businessideas','mentors','businesscoaching','businessmotivation','businessgoals','ventures','skill','innovators','prototype','growthmindset','pitching','innovations','businesslife','businessstrategy','smallbusinessowners','financial','companies','entrepreneurlifestyle','franchise','manufactures','beyourownboss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sma_vec = vect.fit(smartcity)\n",
    "sc100_features['sma_bow'] = sc100_features['clean_text'].apply(lambda x: np.sum(sma_vec.transform([x]).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "civ_vec = vect.fit(civictech)\n",
    "sc100_features['civ_bow'] = sc100_features['clean_text'].apply(lambda x: np.sum(civ_vec.transform([x]).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_vec = vect.fit(infrastructure)\n",
    "sc100_features['inf_bow'] = sc100_features['clean_text'].apply(lambda x: np.sum(inf_vec.transform([x]).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sus_vec = vect.fit(sustainability)\n",
    "sc100_features['sus_bow'] = sc100_features['clean_text'].apply(lambda x: np.sum(sus_vec.transform([x]).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_vec = vect.fit(governance)\n",
    "sc100_features['gov_bow'] = sc100_features['clean_text'].apply(lambda x: np.sum(gov_vec.transform([x]).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_vec = vect.fit(entrepreneurship)\n",
    "sc100_features['ent_bow'] = sc100_features['clean_text'].apply(lambda x: np.sum(ent_vec.transform([x]).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg smartcity weight\n",
    "sc100_features['avg_sma_bow'] = sc100_features['sma_bow']/sc100_features['num_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg civictech weight\n",
    "sc100_features['avg_civ_bow'] = sc100_features['civ_bow']/sc100_features['num_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg infrastructure weight\n",
    "sc100_features['avg_inf_bow'] = sc100_features['inf_bow']/sc100_features['num_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg sustainability weight\n",
    "sc100_features['avg_sus_bow'] = sc100_features['sus_bow']/sc100_features['num_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg governance weight\n",
    "sc100_features['avg_gov_bow'] = sc100_features['gov_bow']/sc100_features['num_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg entrepreneurship weight\n",
    "sc100_features['avg_ent_bow'] = sc100_features['ent_bow']/sc100_features['num_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_id</th>\n",
       "      <th>rank2020</th>\n",
       "      <th>num_tweets</th>\n",
       "      <th>avg_tweet_len</th>\n",
       "      <th>avg_num_words</th>\n",
       "      <th>avg_stopwords</th>\n",
       "      <th>avg_num_sentences</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>avg_punctuation</th>\n",
       "      <th>avg_hashtags</th>\n",
       "      <th>avg_numerics</th>\n",
       "      <th>avg_upper</th>\n",
       "      <th>avg_neg</th>\n",
       "      <th>avg_neu</th>\n",
       "      <th>avg_pos</th>\n",
       "      <th>avg_compound</th>\n",
       "      <th>avg_polarity</th>\n",
       "      <th>avg_subjectivity</th>\n",
       "      <th>tweets_merged</th>\n",
       "      <th>words_total</th>\n",
       "      <th>stopwords_total</th>\n",
       "      <th>punctuation_total</th>\n",
       "      <th>hashtags_total</th>\n",
       "      <th>numerics_total</th>\n",
       "      <th>upper_total</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_words_total</th>\n",
       "      <th>clean_char_total</th>\n",
       "      <th>avg_clean_word_len</th>\n",
       "      <th>polarity_clean</th>\n",
       "      <th>subjectivity_clean</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>most_freq100</th>\n",
       "      <th>count_vec</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>sma_bow</th>\n",
       "      <th>civ_bow</th>\n",
       "      <th>inf_bow</th>\n",
       "      <th>sus_bow</th>\n",
       "      <th>gov_bow</th>\n",
       "      <th>ent_bow</th>\n",
       "      <th>avg_sma_bow</th>\n",
       "      <th>avg_civ_bow</th>\n",
       "      <th>avg_inf_bow</th>\n",
       "      <th>avg_sus_bow</th>\n",
       "      <th>avg_gov_bow</th>\n",
       "      <th>avg_ent_bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abudhabi</td>\n",
       "      <td>42</td>\n",
       "      <td>832</td>\n",
       "      <td>193.930288</td>\n",
       "      <td>25.002404</td>\n",
       "      <td>5.794471</td>\n",
       "      <td>1.435096</td>\n",
       "      <td>7.076026</td>\n",
       "      <td>14.016827</td>\n",
       "      <td>4.602163</td>\n",
       "      <td>0.278846</td>\n",
       "      <td>1.091346</td>\n",
       "      <td>0.021933</td>\n",
       "      <td>0.860132</td>\n",
       "      <td>0.117936</td>\n",
       "      <td>0.299883</td>\n",
       "      <td>0.189999</td>\n",
       "      <td>0.349085</td>\n",
       "      <td>'A Delegation from  @aau_ae visited @BurjeelMe...</td>\n",
       "      <td>166602</td>\n",
       "      <td>4811</td>\n",
       "      <td>15703</td>\n",
       "      <td>3268</td>\n",
       "      <td>202</td>\n",
       "      <td>760</td>\n",
       "      <td>delegation aau_ae visited burjeelmedicity one ...</td>\n",
       "      <td>14484</td>\n",
       "      <td>173030</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.242483</td>\n",
       "      <td>0.471562</td>\n",
       "      <td>[delegation, aau_ae, visited, burjeelmedicity,...</td>\n",
       "      <td>[(delegation, NN), (aau_ae, NN), (visited, VBD...</td>\n",
       "      <td>[(abudhabi, 677), (uae, 224), (abu, 156), (dub...</td>\n",
       "      <td>(0, 1235)\\t1\\n  (1, 2)\\t1\\n  (2, 5825)\\t1\\n ...</td>\n",
       "      <td>(0, 1235)\\t1.0\\n  (1, 2)\\t1.0\\n  (2, 5825)\\t...</td>\n",
       "      <td>309</td>\n",
       "      <td>296</td>\n",
       "      <td>335</td>\n",
       "      <td>257</td>\n",
       "      <td>310</td>\n",
       "      <td>335</td>\n",
       "      <td>0.371394</td>\n",
       "      <td>0.355769</td>\n",
       "      <td>0.402644</td>\n",
       "      <td>0.308894</td>\n",
       "      <td>0.372596</td>\n",
       "      <td>0.402644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abuja</td>\n",
       "      <td>107</td>\n",
       "      <td>828</td>\n",
       "      <td>209.646135</td>\n",
       "      <td>27.042271</td>\n",
       "      <td>5.580918</td>\n",
       "      <td>1.437198</td>\n",
       "      <td>7.075491</td>\n",
       "      <td>15.369565</td>\n",
       "      <td>5.787440</td>\n",
       "      <td>0.404589</td>\n",
       "      <td>1.263285</td>\n",
       "      <td>0.018193</td>\n",
       "      <td>0.856400</td>\n",
       "      <td>0.125396</td>\n",
       "      <td>0.356573</td>\n",
       "      <td>0.186980</td>\n",
       "      <td>0.365899</td>\n",
       "      <td>'Brother #Abuja https://t.co/6J3yOIP8Jm', 'Our...</td>\n",
       "      <td>180295</td>\n",
       "      <td>4561</td>\n",
       "      <td>18553</td>\n",
       "      <td>3862</td>\n",
       "      <td>232</td>\n",
       "      <td>766</td>\n",
       "      <td>brother abuja httpsSkeptical_annoyed_undecided...</td>\n",
       "      <td>15083</td>\n",
       "      <td>182674</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.249437</td>\n",
       "      <td>0.498789</td>\n",
       "      <td>[brother, abuja, httpsSkeptical_annoyed_undeci...</td>\n",
       "      <td>[(brother, NN), (abuja, NN), (httpsSkeptical_a...</td>\n",
       "      <td>[(abuja, 635), (lagos, 163), (abujatwittercomm...</td>\n",
       "      <td>(0, 732)\\t1\\n  (1, 24)\\t1\\n  (2, 2539)\\t1\\n ...</td>\n",
       "      <td>(0, 732)\\t1.0\\n  (1, 24)\\t1.0\\n  (2, 2539)\\t...</td>\n",
       "      <td>247</td>\n",
       "      <td>201</td>\n",
       "      <td>296</td>\n",
       "      <td>67</td>\n",
       "      <td>290</td>\n",
       "      <td>398</td>\n",
       "      <td>0.298309</td>\n",
       "      <td>0.242754</td>\n",
       "      <td>0.357488</td>\n",
       "      <td>0.080918</td>\n",
       "      <td>0.350242</td>\n",
       "      <td>0.480676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amsterdam</td>\n",
       "      <td>9</td>\n",
       "      <td>1000</td>\n",
       "      <td>180.311000</td>\n",
       "      <td>22.627000</td>\n",
       "      <td>5.220000</td>\n",
       "      <td>1.520000</td>\n",
       "      <td>7.579219</td>\n",
       "      <td>14.554000</td>\n",
       "      <td>5.239000</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>0.663000</td>\n",
       "      <td>0.024567</td>\n",
       "      <td>0.875009</td>\n",
       "      <td>0.100439</td>\n",
       "      <td>0.250798</td>\n",
       "      <td>0.152442</td>\n",
       "      <td>0.316410</td>\n",
       "      <td>'Herfst in #Amsterdam 🍂🍃🍁 https://t.co/F0VKRmq...</td>\n",
       "      <td>185912</td>\n",
       "      <td>5200</td>\n",
       "      <td>18994</td>\n",
       "      <td>4740</td>\n",
       "      <td>134</td>\n",
       "      <td>563</td>\n",
       "      <td>herfst amsterdam fallen_leafleaf_fluttering_in...</td>\n",
       "      <td>15579</td>\n",
       "      <td>197584</td>\n",
       "      <td>11.9</td>\n",
       "      <td>0.218700</td>\n",
       "      <td>0.476012</td>\n",
       "      <td>[herfst, amsterdam, fallen_leafleaf_fluttering...</td>\n",
       "      <td>[(herfst, NN), (amsterdam, NN), (fallen_leafle...</td>\n",
       "      <td>[(amsterdam, 993), (netherlands, 80), (day, 64...</td>\n",
       "      <td>(0, 1939)\\t1\\n  (1, 171)\\t1\\n  (2, 1517)\\t1\\...</td>\n",
       "      <td>(0, 1939)\\t1.0\\n  (1, 171)\\t1.0\\n  (2, 1517)...</td>\n",
       "      <td>391</td>\n",
       "      <td>253</td>\n",
       "      <td>348</td>\n",
       "      <td>105</td>\n",
       "      <td>212</td>\n",
       "      <td>295</td>\n",
       "      <td>0.391000</td>\n",
       "      <td>0.253000</td>\n",
       "      <td>0.348000</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.212000</td>\n",
       "      <td>0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ankara</td>\n",
       "      <td>57</td>\n",
       "      <td>313</td>\n",
       "      <td>185.808307</td>\n",
       "      <td>22.984026</td>\n",
       "      <td>3.974441</td>\n",
       "      <td>1.306709</td>\n",
       "      <td>7.510589</td>\n",
       "      <td>14.747604</td>\n",
       "      <td>5.623003</td>\n",
       "      <td>0.338658</td>\n",
       "      <td>0.840256</td>\n",
       "      <td>0.039681</td>\n",
       "      <td>0.865914</td>\n",
       "      <td>0.094422</td>\n",
       "      <td>0.192013</td>\n",
       "      <td>0.128041</td>\n",
       "      <td>0.241076</td>\n",
       "      <td>'I had a great and very joyful flight today ✈️...</td>\n",
       "      <td>60230</td>\n",
       "      <td>1241</td>\n",
       "      <td>6328</td>\n",
       "      <td>1426</td>\n",
       "      <td>77</td>\n",
       "      <td>224</td>\n",
       "      <td>great joyful flight today airplane nto ankara ...</td>\n",
       "      <td>5302</td>\n",
       "      <td>66000</td>\n",
       "      <td>11.7</td>\n",
       "      <td>0.164460</td>\n",
       "      <td>0.419755</td>\n",
       "      <td>[great, joyful, flight, today, airplane, nto, ...</td>\n",
       "      <td>[(great, JJ), (joyful, JJ), (flight, NN), (tod...</td>\n",
       "      <td>[(ankara, 251), (turkey, 77), (turkish, 26), (...</td>\n",
       "      <td>(0, 1059)\\t1\\n  (1, 1592)\\t1\\n  (2, 952)\\t1\\...</td>\n",
       "      <td>(0, 1059)\\t1.0\\n  (1, 1592)\\t1.0\\n  (2, 952)...</td>\n",
       "      <td>49</td>\n",
       "      <td>73</td>\n",
       "      <td>67</td>\n",
       "      <td>20</td>\n",
       "      <td>106</td>\n",
       "      <td>104</td>\n",
       "      <td>0.156550</td>\n",
       "      <td>0.233227</td>\n",
       "      <td>0.214058</td>\n",
       "      <td>0.063898</td>\n",
       "      <td>0.338658</td>\n",
       "      <td>0.332268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>athens</td>\n",
       "      <td>99</td>\n",
       "      <td>1000</td>\n",
       "      <td>186.752000</td>\n",
       "      <td>24.268000</td>\n",
       "      <td>5.405000</td>\n",
       "      <td>1.503000</td>\n",
       "      <td>7.170592</td>\n",
       "      <td>15.201000</td>\n",
       "      <td>5.442000</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.042164</td>\n",
       "      <td>0.865646</td>\n",
       "      <td>0.092190</td>\n",
       "      <td>0.176392</td>\n",
       "      <td>0.126462</td>\n",
       "      <td>0.320793</td>\n",
       "      <td>'Georgia GA #GARunoffs\\n#Sparta #Athens #Ciara...</td>\n",
       "      <td>192839</td>\n",
       "      <td>5370</td>\n",
       "      <td>20229</td>\n",
       "      <td>4795</td>\n",
       "      <td>180</td>\n",
       "      <td>766</td>\n",
       "      <td>georgia ga garunoffsnsparta athens ciara lukes...</td>\n",
       "      <td>16782</td>\n",
       "      <td>204722</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.173722</td>\n",
       "      <td>0.448072</td>\n",
       "      <td>[georgia, ga, garunoffsnsparta, athens, ciara,...</td>\n",
       "      <td>[(georgia, NN), (ga, NN), (garunoffsnsparta, N...</td>\n",
       "      <td>[(athens, 867), (greece, 322), (greek, 74), (a...</td>\n",
       "      <td>(0, 1653)\\t1\\n  (1, 1613)\\t1\\n  (2, 1628)\\t1...</td>\n",
       "      <td>(0, 1653)\\t1.0\\n  (1, 1613)\\t1.0\\n  (2, 1628...</td>\n",
       "      <td>248</td>\n",
       "      <td>324</td>\n",
       "      <td>265</td>\n",
       "      <td>103</td>\n",
       "      <td>265</td>\n",
       "      <td>317</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.324000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.317000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     city_id rank2020  num_tweets  avg_tweet_len  avg_num_words  \\\n",
       "0   abudhabi       42         832     193.930288      25.002404   \n",
       "1      abuja      107         828     209.646135      27.042271   \n",
       "2  amsterdam        9        1000     180.311000      22.627000   \n",
       "3     ankara       57         313     185.808307      22.984026   \n",
       "4     athens       99        1000     186.752000      24.268000   \n",
       "\n",
       "   avg_stopwords  avg_num_sentences  avg_word_len  avg_punctuation  \\\n",
       "0       5.794471           1.435096      7.076026        14.016827   \n",
       "1       5.580918           1.437198      7.075491        15.369565   \n",
       "2       5.220000           1.520000      7.579219        14.554000   \n",
       "3       3.974441           1.306709      7.510589        14.747604   \n",
       "4       5.405000           1.503000      7.170592        15.201000   \n",
       "\n",
       "   avg_hashtags  avg_numerics  avg_upper   avg_neg   avg_neu   avg_pos  \\\n",
       "0      4.602163      0.278846   1.091346  0.021933  0.860132  0.117936   \n",
       "1      5.787440      0.404589   1.263285  0.018193  0.856400  0.125396   \n",
       "2      5.239000      0.153000   0.663000  0.024567  0.875009  0.100439   \n",
       "3      5.623003      0.338658   0.840256  0.039681  0.865914  0.094422   \n",
       "4      5.442000      0.226000   0.865000  0.042164  0.865646  0.092190   \n",
       "\n",
       "   avg_compound  avg_polarity  avg_subjectivity  \\\n",
       "0      0.299883      0.189999          0.349085   \n",
       "1      0.356573      0.186980          0.365899   \n",
       "2      0.250798      0.152442          0.316410   \n",
       "3      0.192013      0.128041          0.241076   \n",
       "4      0.176392      0.126462          0.320793   \n",
       "\n",
       "                                       tweets_merged  words_total  \\\n",
       "0  'A Delegation from  @aau_ae visited @BurjeelMe...       166602   \n",
       "1  'Brother #Abuja https://t.co/6J3yOIP8Jm', 'Our...       180295   \n",
       "2  'Herfst in #Amsterdam 🍂🍃🍁 https://t.co/F0VKRmq...       185912   \n",
       "3  'I had a great and very joyful flight today ✈️...        60230   \n",
       "4  'Georgia GA #GARunoffs\\n#Sparta #Athens #Ciara...       192839   \n",
       "\n",
       "   stopwords_total  punctuation_total  hashtags_total  numerics_total  \\\n",
       "0             4811              15703            3268             202   \n",
       "1             4561              18553            3862             232   \n",
       "2             5200              18994            4740             134   \n",
       "3             1241               6328            1426              77   \n",
       "4             5370              20229            4795             180   \n",
       "\n",
       "   upper_total                                         clean_text  \\\n",
       "0          760  delegation aau_ae visited burjeelmedicity one ...   \n",
       "1          766  brother abuja httpsSkeptical_annoyed_undecided...   \n",
       "2          563  herfst amsterdam fallen_leafleaf_fluttering_in...   \n",
       "3          224  great joyful flight today airplane nto ankara ...   \n",
       "4          766  georgia ga garunoffsnsparta athens ciara lukes...   \n",
       "\n",
       "   clean_words_total  clean_char_total  avg_clean_word_len  polarity_clean  \\\n",
       "0              14484            173030                11.2        0.242483   \n",
       "1              15083            182674                11.4        0.249437   \n",
       "2              15579            197584                11.9        0.218700   \n",
       "3               5302             66000                11.7        0.164460   \n",
       "4              16782            204722                11.5        0.173722   \n",
       "\n",
       "   subjectivity_clean                                             tokens  \\\n",
       "0            0.471562  [delegation, aau_ae, visited, burjeelmedicity,...   \n",
       "1            0.498789  [brother, abuja, httpsSkeptical_annoyed_undeci...   \n",
       "2            0.476012  [herfst, amsterdam, fallen_leafleaf_fluttering...   \n",
       "3            0.419755  [great, joyful, flight, today, airplane, nto, ...   \n",
       "4            0.448072  [georgia, ga, garunoffsnsparta, athens, ciara,...   \n",
       "\n",
       "                                             pos_tag  \\\n",
       "0  [(delegation, NN), (aau_ae, NN), (visited, VBD...   \n",
       "1  [(brother, NN), (abuja, NN), (httpsSkeptical_a...   \n",
       "2  [(herfst, NN), (amsterdam, NN), (fallen_leafle...   \n",
       "3  [(great, JJ), (joyful, JJ), (flight, NN), (tod...   \n",
       "4  [(georgia, NN), (ga, NN), (garunoffsnsparta, N...   \n",
       "\n",
       "                                        most_freq100  \\\n",
       "0  [(abudhabi, 677), (uae, 224), (abu, 156), (dub...   \n",
       "1  [(abuja, 635), (lagos, 163), (abujatwittercomm...   \n",
       "2  [(amsterdam, 993), (netherlands, 80), (day, 64...   \n",
       "3  [(ankara, 251), (turkey, 77), (turkish, 26), (...   \n",
       "4  [(athens, 867), (greece, 322), (greek, 74), (a...   \n",
       "\n",
       "                                           count_vec  \\\n",
       "0    (0, 1235)\\t1\\n  (1, 2)\\t1\\n  (2, 5825)\\t1\\n ...   \n",
       "1    (0, 732)\\t1\\n  (1, 24)\\t1\\n  (2, 2539)\\t1\\n ...   \n",
       "2    (0, 1939)\\t1\\n  (1, 171)\\t1\\n  (2, 1517)\\t1\\...   \n",
       "3    (0, 1059)\\t1\\n  (1, 1592)\\t1\\n  (2, 952)\\t1\\...   \n",
       "4    (0, 1653)\\t1\\n  (1, 1613)\\t1\\n  (2, 1628)\\t1...   \n",
       "\n",
       "                                               tfidf  sma_bow  civ_bow  \\\n",
       "0    (0, 1235)\\t1.0\\n  (1, 2)\\t1.0\\n  (2, 5825)\\t...      309      296   \n",
       "1    (0, 732)\\t1.0\\n  (1, 24)\\t1.0\\n  (2, 2539)\\t...      247      201   \n",
       "2    (0, 1939)\\t1.0\\n  (1, 171)\\t1.0\\n  (2, 1517)...      391      253   \n",
       "3    (0, 1059)\\t1.0\\n  (1, 1592)\\t1.0\\n  (2, 952)...       49       73   \n",
       "4    (0, 1653)\\t1.0\\n  (1, 1613)\\t1.0\\n  (2, 1628...      248      324   \n",
       "\n",
       "   inf_bow  sus_bow  gov_bow  ent_bow  avg_sma_bow  avg_civ_bow  avg_inf_bow  \\\n",
       "0      335      257      310      335     0.371394     0.355769     0.402644   \n",
       "1      296       67      290      398     0.298309     0.242754     0.357488   \n",
       "2      348      105      212      295     0.391000     0.253000     0.348000   \n",
       "3       67       20      106      104     0.156550     0.233227     0.214058   \n",
       "4      265      103      265      317     0.248000     0.324000     0.265000   \n",
       "\n",
       "   avg_sus_bow  avg_gov_bow  avg_ent_bow  \n",
       "0     0.308894     0.372596     0.402644  \n",
       "1     0.080918     0.350242     0.480676  \n",
       "2     0.105000     0.212000     0.295000  \n",
       "3     0.063898     0.338658     0.332268  \n",
       "4     0.103000     0.265000     0.317000  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc100_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save new .csv file with lastly created columns\n",
    "sc100_features.to_csv('sc100_features.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
